{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53da5f90-e227-44ad-ab11-a0a960bd237d",
   "metadata": {},
   "source": [
    "## Get Company 10K from HTML by accessing EDGAR using a company's ticker and store in firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "021f473d-b6a2-48fb-a4f3-e57d056a44d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "BASE = \"https://data.sec.gov\"\n",
    "HEADERS = {\"User-Agent\": \"kurio-agent/1.0 (potatojacket9@gmail.com)\"}\n",
    "\n",
    "## Step 1. Get CIK for the input ticker symbol~\n",
    "def get_cik(ticker):\n",
    "    \"\"\"Retrieve CIK for a given ticker symbol\"\"\"\n",
    "    res = requests.get(f\"{BASE}/submissions/CIK{ticker}.json\", headers=HEADERS)\n",
    "    if res.status_code != 200:\n",
    "        # fallback: try SEC ticker endpoint\n",
    "        lookup = requests.get(f\"https://www.sec.gov/files/company_tickers.json\", headers=HEADERS).json()\n",
    "        for _, c in lookup.items():\n",
    "            if c[\"ticker\"].lower() == ticker.lower():\n",
    "                return str(c[\"cik_str\"]).zfill(10)\n",
    "    return None\n",
    "\n",
    "## Step 2. Get latest 10K url for the CIK (ticker)\n",
    "def get_latest_10k_url(cik):\n",
    "    \"\"\"Retrieve latest 10-K filing document URL and dates for a given CIK\"\"\"\n",
    "    url = f\"{BASE}/submissions/CIK{cik}.json\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    data = res.json()\n",
    "\n",
    "    for form, acc, filing_date, report_date in zip(\n",
    "        data[\"filings\"][\"recent\"][\"form\"],\n",
    "        data[\"filings\"][\"recent\"][\"accessionNumber\"],\n",
    "        data[\"filings\"][\"recent\"][\"filingDate\"],\n",
    "        data[\"filings\"][\"recent\"][\"reportDate\"]\n",
    "    ):\n",
    "        if form == \"10-K\":\n",
    "            acc_num = acc.replace(\"-\", \"\")\n",
    "            filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_num}/{acc}-index.html\"\n",
    "            return filing_url, filing_date, report_date\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "# 2️⃣ Use your HTML finder (your existing function)\n",
    "def get_primary_html_url(index_url):\n",
    "    \"\"\"\n",
    "    Find the main 10-K HTML document inside the filing's index page.\n",
    "    Returns the actual HTML file URL (not the inline XBRL viewer).\n",
    "    \"\"\"\n",
    "    res = requests.get(index_url, headers=HEADERS)\n",
    "    if res.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch index page: {res.status_code}\")\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"].strip()\n",
    "        text = link.get_text(strip=True).lower()\n",
    "        href_lower = href.lower()\n",
    "\n",
    "        # Skip XMLs and exhibits\n",
    "        if any(x in href_lower for x in [\"_cal.xml\", \"_lab.xml\", \"_pre.xml\", \"_def.xml\", \".xml\"]):\n",
    "            continue\n",
    "        if \"exhibit\" in href_lower or \"ex\" in text:\n",
    "            continue\n",
    "\n",
    "        # Only consider HTML or Inline XBRL\n",
    "        if href_lower.endswith((\".htm\", \".html\")) or \"ix?doc=\" in href_lower:\n",
    "            score = 0\n",
    "            if \"10-k\" in href_lower or \"10k\" in href_lower:\n",
    "                score += 10\n",
    "            if \"ix?doc=\" in href_lower:\n",
    "                score += 20  # inline XBRL links usually indicate the primary document\n",
    "            if \"form\" in text or \"10-k\" in text:\n",
    "                score += 5\n",
    "            candidates.append((score, href))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Pick the top-scoring link\n",
    "    best_href = sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
    "\n",
    "    # Normalize the URL — convert inline XBRL to raw HTML\n",
    "    if \"ix?doc=\" in best_href:\n",
    "        best_href = best_href.split(\"ix?doc=\")[-1]\n",
    "        if not best_href.startswith(\"https://\"):\n",
    "            best_href = \"https://www.sec.gov\" + best_href\n",
    "    elif not best_href.startswith(\"http\"):\n",
    "        if best_href.startswith(\"/\"):\n",
    "            best_href = \"https://www.sec.gov\" + best_href\n",
    "        else:\n",
    "            best_href = \"https://www.sec.gov/\" + best_href\n",
    "\n",
    "    return best_href"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a95f410-5060-4ad0-890b-2f365950331a",
   "metadata": {},
   "source": [
    "## Convert HTML content to markdown text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e01f399-0974-4589-aa78-c298d5e14e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert HTML content to markdown text (also used in notebook: RAG-api)\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import re\n",
    "\n",
    "def clean_10k_html(html_content):\n",
    "    \"\"\"Convert messy 10-K HTML into clean text.\"\"\"\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove hidden and metadata elements\n",
    "    for tag in soup([\"script\", \"style\", \"ix:header\", \"ix:hidden\", \"link\", \"meta\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Keep only the visible content\n",
    "    visible_html = str(soup)\n",
    "\n",
    "    # Convert to readable Markdown-style text\n",
    "    text_maker = html2text.HTML2Text()\n",
    "    text_maker.ignore_links = True\n",
    "    text_maker.ignore_images = True\n",
    "    text_maker.body_width = 0  # No line wrapping\n",
    "    clean_text = text_maker.handle(visible_html)\n",
    "\n",
    "    # Normalize spaces\n",
    "    clean_text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", clean_text)\n",
    "    return clean_text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c67e3d2-4000-4e5a-922f-222d1af18a46",
   "metadata": {},
   "source": [
    "## Given an input ticker and HTML URL location, store into firebase at a specific location (hardwired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fced6a32-ef97-4915-bcc1-21a113f6e29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import requests\n",
    "import os\n",
    "# Make sure you actually load the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def store_10K_text_from_url(ticker, html_url, year):\n",
    "    \"\"\"\n",
    "    Fetch a 10-K HTML from a URL, convert it to cleaned text, and upload\n",
    "    the text to Firebase Storage under:\n",
    "    company_details/EDGAR (US)/filings/{ticker}_10K.txt\n",
    "    \"\"\"\n",
    "\n",
    "    # 1️⃣ Fetch the HTML content\n",
    "    headers = {\"User-Agent\": \"kurio-agent/1.0 (potatojacket9@gmail.com)\"}\n",
    "    res = requests.get(html_url, headers=headers)\n",
    "    if res.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch HTML from SEC: {res.status_code}\")\n",
    "    html_content = res.text\n",
    "\n",
    "    # 2️⃣ Convert HTML to cleaned plain text\n",
    "    try:\n",
    "        text_data = clean_10k_html(html_content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to clean HTML for {ticker}: {e}\")\n",
    "\n",
    "    # 3️⃣ Initialize Firebase Storage client\n",
    "    service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Service account file not found at {service_account_path}\"\n",
    "        )\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    # ✅ Bucket name (must match your Firebase project)\n",
    "    bucket_name = \"funwai-resume.firebasestorage.app\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # 4️⃣ Path inside the bucket (TXT version)\n",
    "    blob_path = f\"company_details/EDGAR (US)/filings/{ticker}_{year}_10K.txt\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    # 5️⃣ Upload the cleaned text\n",
    "    blob.upload_from_string(text_data, content_type=\"text/plain\")\n",
    "\n",
    "    print(f\"✅ Uploaded cleaned 10-K text for {ticker} to {blob_path}\")\n",
    "\n",
    "    # Optionally return a public URL (if your bucket allows)\n",
    "    return blob.public_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2210c1da-2ff6-4eba-a7a4-10164ea19dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/891103/000089110325000027/mtch-20241231.htm\n",
      "✅ Uploaded cleaned 10-K text for MTCH to company_details/EDGAR (US)/filings/MTCH_2024_10K.txt\n",
      "https://storage.googleapis.com/funwai-resume.firebasestorage.app/company_details/EDGAR%20%28US%29/filings/MTCH_2024_10K.txt\n",
      "HTML->text for MTCH stored at: https://www.sec.gov/Archives/edgar/data/891103/000089110325000027/mtch-20241231.htm\n"
     ]
    }
   ],
   "source": [
    "# [Test] Use functions to store 10K as markdown text into firestore\n",
    "ticker = \"MTCH\"\n",
    "cik = get_cik(ticker)\n",
    "filing_url, filing_date, report_date = get_latest_10k_url(cik)\n",
    "html_url = get_primary_html_url(filing_url)\n",
    "print(html_url)\n",
    "data = store_10K_text_from_url(ticker, html_url, report_date[:4])\n",
    "print(data)\n",
    "print(\"HTML->text \" + \"for \" + ticker + \" stored at:\", html_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7549ff9f-d561-42ad-8849-b1f088eaf9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
