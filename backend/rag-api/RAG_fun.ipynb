{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b360eb-c621-48ce-8f37-d7f3d5670d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch HTML from Firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc57ed1-81b8-40be-9ba7-303a2c18ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import firestore\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_firestore_client():\n",
    "    \"\"\"\n",
    "    Initialize a Firestore client using a service account file path\n",
    "    stored in .env as FIREBASE_SERVICE_ACCOUNT_JSON.\n",
    "    \"\"\"\n",
    "    service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Service account file not found. Check FIREBASE_SERVICE_ACCOUNT_JSON in .env: {service_account_path}\"\n",
    "        )\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    db = firestore.Client(credentials=credentials, project=credentials.project_id)\n",
    "    return db\n",
    "\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # ensure .env is loaded for FIREBASE_SERVICE_ACCOUNT_JSON\n",
    "\n",
    "\n",
    "def download_text_from_storage(ticker, service_account_path=None):\n",
    "    \"\"\"\n",
    "    Download cleaned 10-K text content from Firebase Storage for a given ticker.\n",
    "    Path: company_details/EDGAR (US)/filings/{ticker}_10K.txt\n",
    "    \"\"\"\n",
    "\n",
    "    # Get service account path from .env if not provided\n",
    "    if service_account_path is None:\n",
    "        service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(f\"Service account file not found at {service_account_path}\")\n",
    "\n",
    "    # Initialize Firebase Storage client\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    bucket_name = \"funwai-resume.firebasestorage.app\"\n",
    "    file_path = f\"company_details/EDGAR (US)/filings/{ticker}_10K.txt\"\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    # Check if the text file exists\n",
    "    if not blob.exists():\n",
    "        raise FileNotFoundError(f\"File not found for ticker '{ticker}' at path: {file_path}\")\n",
    "\n",
    "    # Download the text content\n",
    "    text_content = blob.download_as_text(encoding=\"utf-8\")\n",
    "\n",
    "    return text_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88273abb-dffc-4231-ad73-e0ad3fb89d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Firebase Storage connection...\n",
      "Service account path: C:/Users/hongn/idealy_new/idealy/backend/funwai-resume-firebase-adminsdk-fbsvc-a956eb6362.json\n",
      "‚ùå Error: File not found for ticker 'AAPL' at path: company_details/EDGAR (US)/filings/AAPL_10K.txt\n"
     ]
    }
   ],
   "source": [
    "# test that we are able to pull data from firestore + firebase storage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç Testing Firebase Storage connection...\")\n",
    "\n",
    "    try:\n",
    "        # Check environment variable\n",
    "        print(\"Service account path:\", os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\"))\n",
    "\n",
    "        # Try to download a known file (update ticker if needed)\n",
    "        ticker = \"AAPL\"\n",
    "        html_content = download_text_from_storage(ticker)\n",
    "        \n",
    "        # Print preview\n",
    "        print(f\"‚úÖ Successfully downloaded 10k (as text) for {ticker}\")\n",
    "        print(\"First 500 characters:\\n\", html_content[:100])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7338282-c1de-4ebf-bd73-8ca644e239ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Create a new OpenAI client with your project key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "## Select an embeddings model:\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "## Select an LLM model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea0b638b-4b70-4417-94e8-2d12242109ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I can't provide real-time weather updates. You can check the current weather in London using a reliable weather website or app.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in London?\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6689807-79af-4f4b-be23-84ee00386012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10k-text-rag\n",
      "[{\n",
      "    \"name\": \"10k-text-rag\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"10k-text-rag-fqh5rav.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 3072,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": null\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone setup\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "print(index_name)\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "print(pc.list_indexes())\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680314a2-4312-47d0-891d-fb8e2d6a6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create index if it does not exist already\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load env\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "\n",
    "# Delete old index if exists\n",
    "if index_name in pc.list_indexes():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "index_name = \"10k-text-rag\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=\"10k-text-rag\",\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dda1a84-a6c8-497d-967a-3485c4e77514",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found for ticker 'ABBV' at path: company_details/EDGAR (US)/filings/ABBV_10K.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ticker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mABBV\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or pass dynamically\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m text_content \u001b[38;5;241m=\u001b[39m download_text_from_storage(ticker)\n\u001b[0;32m      7\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/hongn/idealy_new/idealy/backend/rag-api/clean_10k_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save cleaned text\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m, in \u001b[0;36mdownload_text_from_storage\u001b[1;34m(ticker, service_account_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Check if the text file exists\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found for ticker \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Download the text content\u001b[39;00m\n\u001b[0;32m     61\u001b[0m text_content \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mdownload_as_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File not found for ticker 'ABBV' at path: company_details/EDGAR (US)/filings/ABBV_10K.txt"
     ]
    }
   ],
   "source": [
    "## Convert HTML to text file and store\n",
    "import os\n",
    "\n",
    "ticker = \"ABBV\"  # or pass dynamically\n",
    "text_content = download_text_from_storage(ticker)\n",
    "\n",
    "output_folder = \"C:/Users/hongn/idealy_new/idealy/backend/rag-api/clean_10k_texts\"\n",
    "\n",
    "# Save cleaned text\n",
    "output_path = os.path.join(output_folder, f\"{ticker}_10K.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "print(f\"‚úÖ Saved cleaned text to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f356dfe1-42ae-4a6a-a88d-d5a68119b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 124 documents.\n",
      "\n",
      "First document metadata: {'source': 'clean_10k_texts\\\\AAPL_2025_10K.txt'}\n",
      "\n",
      "--- First 500 characters ---\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "UNITED STATES\n",
      "\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "\n",
      "Washington, D.C. 20549\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "FORM 10-K\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "(Mark One)\n",
      "\n",
      "‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "For the fiscal year ended September 27, 2025\n",
      "\n",
      "or\n",
      "\n",
      "‚òê TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "For the transition period from  to  .\n",
      "\n",
      "Commission File Number: 001-36743\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "Apple Inc.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Path to the folder containing your cleaned text files\n",
    "TEXT_FOLDER = \"./clean_10k_texts\"\n",
    "\n",
    "# Load all .txt files\n",
    "loader = DirectoryLoader(\n",
    "    TEXT_FOLDER,\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} documents.\\n\")\n",
    "\n",
    "# Preview first document\n",
    "print(\"First document metadata:\", docs[0].metadata)\n",
    "print(\"\\n--- First 500 characters ---\\n\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27664e65-0d03-48c7-991f-75d619411ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1 document with 279148 characters.\n",
      "| |   \n",
      "---|---|---  \n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION\n",
      "\n",
      "Washington, D.C. 20549\n",
      "\n",
      "_____________________________________________________________________\n",
      "\n",
      "FORM 10-K\n",
      "\n",
      "_____________________________________________________________________\n",
      "\n",
      "(Mark One)\n",
      "\n",
      "| | | | |   \n",
      "---|---|---|---|---|---  \n",
      "‚òí| ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934  \n",
      "\n",
      "For the fiscal year ended December 31, 2024\n",
      "\n",
      "OR\n",
      "\n",
      "| | | | |   \n",
      "---|---|---|---|---|---  \n",
      "‚òê| TRANSITION REPORT P\n"
     ]
    }
   ],
   "source": [
    "## Load documents\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Path to the folder containing your cleaned text files\n",
    "TEXT_FOLDER = \"./clean_10k_texts\"  # e.g., output of clean_10k_html()\n",
    "\n",
    "# Option 1: Load a single file\n",
    "single_file_path = os.path.join(TEXT_FOLDER, \"NFLX_10K.txt\")\n",
    "loader = TextLoader(single_file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded 1 document with {len(docs[0].page_content)} characters.\")\n",
    "print(docs[0].page_content[:500])\n",
    "\n",
    "# Option 2: Load all text files in a folder\n",
    "# loader = DirectoryLoader(TEXT_FOLDER, glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "# docs = loader.load()\n",
    "\n",
    "# print(f\"‚úÖ Loaded {len(docs)} documents.\")\n",
    "# print(docs[0].metadata)\n",
    "# print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "292a8949-c436-495b-b48c-872cdf9f35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split into 93427 chunks.\n"
     ]
    }
   ],
   "source": [
    "## Docs to Chunks via textsplitter functions\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    add_start_index=True)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "656a42a9-6a41-4bec-9ee3-29164ba74317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 314/468 [25:49<12:17,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 314, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:47 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '96', 'x-pinecone-request-id': '7982488119216157449', 'x-envoy-upstream-service-time': '28', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 314, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '66', 'x-pinecone-request-id': '1847917596060236855', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 314, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '79', 'x-pinecone-request-id': '7812485040887677095', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 315/468 [26:07<22:43,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 315, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:03 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '99', 'x-pinecone-request-id': '6887225196364178439', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 315, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '163', 'x-pinecone-request-id': '1152302126835574217', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 315, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '67', 'x-pinecone-request-id': '5751893401277074809', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 316/468 [26:26<29:52, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 316, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:22 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '74', 'x-pinecone-request-id': '3443433195536649323', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 316, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '81', 'x-pinecone-request-id': '4455966097111190929', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 316, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '119', 'x-pinecone-request-id': '4187682213216210893', 'x-envoy-upstream-service-time': '9', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 317/468 [26:46<36:27, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 317, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '79', 'x-pinecone-request-id': '5074216660346096774', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 317, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '85', 'x-pinecone-request-id': '4451538800431435109', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 317, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '72', 'x-pinecone-request-id': '8232850220250487554', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 318/468 [27:06<40:20, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 318, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '103', 'x-pinecone-request-id': '4980604788259433491', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 318, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:11 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '64', 'x-pinecone-request-id': '3450590172140288414', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 318, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '62', 'x-pinecone-request-id': '1277798384400842462', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 319/468 [27:27<43:21, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 319, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '107', 'x-pinecone-request-id': '5810758915277509526', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 319, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '110', 'x-pinecone-request-id': '8702142190872492001', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 319, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '98', 'x-pinecone-request-id': '796670792108520295', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 320/468 [27:45<43:39, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 320, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '141', 'x-pinecone-request-id': '2918533118264848584', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 320, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '115', 'x-pinecone-request-id': '217203292585814881', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 320, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:54 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '70', 'x-pinecone-request-id': '2654920316656810352', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 321/468 [28:04<44:09, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 321, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '95', 'x-pinecone-request-id': '2395756261778790895', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 321, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '105', 'x-pinecone-request-id': '5033429704561069676', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 321, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '76', 'x-pinecone-request-id': '3084926058212948187', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 322/468 [28:23<45:00, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 322, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '63', 'x-pinecone-request-id': '7884871746501029296', 'x-envoy-upstream-service-time': '7', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 322, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '59', 'x-pinecone-request-id': '4345499180109554876', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 322, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '101', 'x-pinecone-request-id': '4091295665732925367', 'x-envoy-upstream-service-time': '9', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 323/468 [28:43<45:12, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 323, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '113', 'x-pinecone-request-id': '1783150408390401669', 'x-envoy-upstream-service-time': '7', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 323, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '91', 'x-pinecone-request-id': '957619681063049265', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 323, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '64', 'x-pinecone-request-id': '1648590854806108805', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 324/468 [29:02<45:02, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 324, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:58 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '128', 'x-pinecone-request-id': '7256124030153667348', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 324, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '70', 'x-pinecone-request-id': '7280234316967944236', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 324, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:10 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '114', 'x-pinecone-request-id': '6012754374071021034', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 325/468 [29:20<44:46, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 325, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '110', 'x-pinecone-request-id': '7934570794545337204', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 325, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:24 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '92', 'x-pinecone-request-id': '6497272980675131265', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 325/468 [29:37<13:02,  5.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         ids \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[0;32m     14\u001b[0m         document_ids\u001b[38;5;241m.\u001b[39mextend(ids)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# break retry loop\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:258\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    257\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    259\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:362\u001b[0m, in \u001b[0;36mPineconeVectorStore.add_texts\u001b[1;34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m vector_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunk_ids, embeddings, chunk_metadatas))\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     async_res \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 362\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    363\u001b[0m             vectors\u001b[38;5;241m=\u001b[39mbatch_vector_tuples,\n\u001b[0;32m    364\u001b[0m             namespace\u001b[38;5;241m=\u001b[39mnamespace,\n\u001b[0;32m    365\u001b[0m             async_req\u001b[38;5;241m=\u001b[39masync_req,\n\u001b[0;32m    366\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    367\u001b[0m         )\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[0;32m    369\u001b[0m     ]\n\u001b[0;32m    370\u001b[0m     [res\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m async_res]\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\utils\\error_handling.py:15\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Lazy import of urllib3 exceptions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxRetryError, ProtocolError \u001b[38;5;28;01mas\u001b[39;00m Urllib3ProtocolError\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\index.py:212\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_req is not supported when batch_size is provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upsert in parallel, please follow: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.pinecone.io/docs/insert-data#sending-upserts-in-parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsert_batch(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\index.py:239\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert_batch\u001b[39m(\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    231\u001b[0m     vectors: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    237\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m UpsertResponse:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert_vectors(\n\u001b[1;32m--> 239\u001b[0m         IndexRequestFactory\u001b[38;5;241m.\u001b[39mupsert_request(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_openapi_kwargs(kwargs),\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\request_factory.py:93\u001b[0m, in \u001b[0;36mIndexRequestFactory.upsert_request\u001b[1;34m(vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvec_builder\u001b[39m(v):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UpsertRequest(\n\u001b[1;32m---> 93\u001b[0m     vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[0;32m     95\u001b[0m     _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnon_openapi_kwargs(kwargs),\n\u001b[0;32m     97\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\request_factory.py:90\u001b[0m, in \u001b[0;36mIndexRequestFactory.upsert_request.<locals>.vec_builder\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvec_builder\u001b[39m(v):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\vector_factory.py:48\u001b[0m, in \u001b[0;36mVectorFactory.build\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenApiVector(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_tuple_to_vector(item, check_type)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Mapping):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_dict_to_vector(item, check_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\vector_factory.py:64\u001b[0m, in \u001b[0;36mVectorFactory._tuple_to_vector\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse values are not supported in tuples. Please use either dicts or OpenApiVector objects as inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenApiVector(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m     66\u001b[0m         values\u001b[38;5;241m=\u001b[39mconvert_to_list(values),\n\u001b[0;32m     67\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m     68\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39mcheck_type,\n\u001b[0;32m     69\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:36\u001b[0m, in \u001b[0;36mconvert_js_args_to_python_args.<locals>.wrapped_init\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_property_naming:\n\u001b[0;32m     33\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m change_keys_js_to_python(\n\u001b[0;32m     34\u001b[0m         kwargs, _self \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_self, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _self\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\core\\openapi\\db_data\\model\\vector.py:294\u001b[0m, in \u001b[0;36mVector.__init__\u001b[1;34m(self, id, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    287\u001b[0m     var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_map\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configuration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m ):\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# discard variable.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_name, var_value)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_only_vars:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiAttributeError(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is a read-only attribute. Use `from_openapi_data` to instantiate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass with read only attributes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:171\u001b[0m, in \u001b[0;36mOpenApiModel.__setattr__\u001b[1;34m(self, attr, value)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, value):\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"set the value of an attribute using dot notation: `instance.attr = val`\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m[attr] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:456\u001b[0m, in \u001b[0;36mModelNormal.__setitem__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attribute(name, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:139\u001b[0m, in \u001b[0;36mOpenApiModel.set_attribute\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiTypeError(\n\u001b[0;32m    135\u001b[0m         error_msg, path_to_item\u001b[38;5;241m=\u001b[39mpath_to_item, valid_classes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mstr\u001b[39m,), key_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_type:\n\u001b[1;32m--> 139\u001b[0m     value \u001b[38;5;241m=\u001b[39m validate_and_convert_types(\n\u001b[0;32m    140\u001b[0m         value,\n\u001b[0;32m    141\u001b[0m         required_types_mixed,\n\u001b[0;32m    142\u001b[0m         path_to_item,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_property_naming,\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_type,\n\u001b[0;32m    145\u001b[0m         configuration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configuration,\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (name,) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_values \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_allowed_values:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# Disabling allowed_value validation on response makes the SDK\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# less fragile if unexpected values are returned. For example, if\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# an unexpected index status is returned, we don't want to break\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# when listing indexes due to validation on the status field against\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# the allowed values in the enum.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     check_allowed_values(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_values, (name,), value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:1548\u001b[0m, in \u001b[0;36mvalidate_and_convert_types\u001b[1;34m(input_value, required_types_mixed, path_to_item, spec_property_naming, _check_type, configuration)\u001b[0m\n\u001b[0;32m   1546\u001b[0m         inner_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(path_to_item)\n\u001b[0;32m   1547\u001b[0m         inner_path\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[1;32m-> 1548\u001b[0m         input_value[index] \u001b[38;5;241m=\u001b[39m validate_and_convert_types(\n\u001b[0;32m   1549\u001b[0m             inner_value,\n\u001b[0;32m   1550\u001b[0m             inner_required_types,\n\u001b[0;32m   1551\u001b[0m             inner_path,\n\u001b[0;32m   1552\u001b[0m             spec_property_naming,\n\u001b[0;32m   1553\u001b[0m             _check_type,\n\u001b[0;32m   1554\u001b[0m             configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[0;32m   1555\u001b[0m         )\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_value \u001b[38;5;241m==\u001b[39m {}:\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;66;03m# allow an empty dict\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## load chunks into the Pinecone Vector store ('vector_store' defined earlier)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 200  # You can tune this (100‚Äì300 is ideal)\n",
    "document_ids = []\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Uploading chunks\"):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "\n",
    "    # Retry logic (recommended for Pinecone)\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            ids = vector_store.add_documents(documents=batch)\n",
    "            document_ids.extend(ids)\n",
    "            break  # break retry loop\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error uploading batch {i//BATCH_SIZE}, attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)  # wait + retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e736858-7839-4eda-8e92-26d6245aa8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(document_ids))\n",
    "vector_store.similarity_search(\"What does Apple say about revenue recognition?\", k=2)\n",
    "# the above returns 2 relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "447d8f8a-1b6a-4863-82e3-456c95aca04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2765ea43-32e4-4ea6-a6fb-08b4c06d3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Example using local API key or .env\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb6cb450-db22-4f96-bfc6-2673714d44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# System prompt / instructions for the agent\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant that answers user queries using the provided context. \"\n",
    "    \"Use the retrieval tool when necessary.\"\n",
    "    \"provide succinct answers and as quantitative answers as available from retrieval\"\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_agent(llm, tools=tools, system_prompt=system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c74d1e2-cc6e-42b2-ac64-37c6cb58d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Morgan Stanley makes money primarily through the following business segments:\n",
      "\n",
      "1. Investment Banking: Revenue from financial advisory and underwriting assignments.\n",
      "2. Investment Management: Revenue from providing asset management and wealth advisory services.\n",
      "3. Commissions and Fees: Revenue from executing and clearing client transactions on stock, options, and futures exchanges, as well as over-the-counter (OTC) transactions.\n",
      "4. Markets: Revenue from trading activities, including equity and fixed income markets.\n",
      "\n",
      "Products and Services & Revenue (latest available):\n",
      "- Markets (trading, including equities and fixed income): $19.8 billion in revenue.\n",
      "  - Equity Markets: Growth driven by cash equities, equity derivatives, and prime services.\n",
      "  - Fixed Income Markets: Growth in asset-backed financing, securitization, and underwriting fees.\n",
      "- Investment Banking and Investment Management: Specific revenue figures for these segments are not provided in the retrieved context, but they are major contributors.\n",
      "\n",
      "Main Competitors:\n",
      "Morgan Stanley‚Äôs main competitors include:\n",
      "- Goldman Sachs\n",
      "- JPMorgan Chase\n",
      "- Bank of America Merrill Lynch\n",
      "- Citigroup\n",
      "- Wells Fargo\n",
      "- Other global and regional investment banks, asset managers, and financial technology firms.\n",
      "\n",
      "Let me know if you need more detailed revenue breakdowns or information on a specific product or service.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How does Morgan Stanley make money??\\n\\n\"\n",
    "    \"Once you get the answer, please find some of the products and services they sell and how much money each product made.\\n\\n\"\n",
    "    \"Who are Morgan Stanley's main competitors?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    msg = event[\"messages\"][-1]\n",
    "\n",
    "    # Only print the final AI messages\n",
    "    if getattr(msg, \"type\", None) == \"ai\": # do not get human, tool messages, only the AI response\n",
    "        print(msg.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cb434-c32c-47df-ad1a-f804b4226b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
