{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b360eb-c621-48ce-8f37-d7f3d5670d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch HTML from Firestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cc57ed1-81b8-40be-9ba7-303a2c18ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import firestore\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "def get_firestore_client():\n",
    "    \"\"\"\n",
    "    Initialize a Firestore client using a service account file path\n",
    "    stored in .env as FIREBASE_SERVICE_ACCOUNT_JSON.\n",
    "    \"\"\"\n",
    "    service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Service account file not found. Check FIREBASE_SERVICE_ACCOUNT_JSON in .env: {service_account_path}\"\n",
    "        )\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    db = firestore.Client(credentials=credentials, project=credentials.project_id)\n",
    "    return db\n",
    "\n",
    "\n",
    "import os\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # ensure .env is loaded for FIREBASE_SERVICE_ACCOUNT_JSON\n",
    "\n",
    "\n",
    "def download_text_from_storage(ticker, service_account_path=None):\n",
    "    \"\"\"\n",
    "    Download cleaned 10-K text content from Firebase Storage for a given ticker.\n",
    "    Path: company_details/EDGAR (US)/filings/{ticker}_10K.txt\n",
    "    \"\"\"\n",
    "\n",
    "    # Get service account path from .env if not provided\n",
    "    if service_account_path is None:\n",
    "        service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(f\"Service account file not found at {service_account_path}\")\n",
    "\n",
    "    # Initialize Firebase Storage client\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    bucket_name = \"funwai-resume.firebasestorage.app\"\n",
    "    file_path = f\"company_details/EDGAR (US)/filings/{ticker}_10K.txt\"\n",
    "\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    # Check if the text file exists\n",
    "    if not blob.exists():\n",
    "        raise FileNotFoundError(f\"File not found for ticker '{ticker}' at path: {file_path}\")\n",
    "\n",
    "    # Download the text content\n",
    "    text_content = blob.download_as_text(encoding=\"utf-8\")\n",
    "\n",
    "    return text_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88273abb-dffc-4231-ad73-e0ad3fb89d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing Firebase Storage connection...\n",
      "Service account path: C:/Users/hongn/idealy_new/idealy/backend/funwai-resume-firebase-adminsdk-fbsvc-a956eb6362.json\n",
      "‚ùå Error: File not found for ticker 'AAPL' at path: company_details/EDGAR (US)/filings/AAPL_10K.txt\n"
     ]
    }
   ],
   "source": [
    "# test that we are able to pull data from firestore + firebase storage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üîç Testing Firebase Storage connection...\")\n",
    "\n",
    "    try:\n",
    "        # Check environment variable\n",
    "        print(\"Service account path:\", os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\"))\n",
    "\n",
    "        # Try to download a known file (update ticker if needed)\n",
    "        ticker = \"AAPL\"\n",
    "        html_content = download_text_from_storage(ticker)\n",
    "        \n",
    "        # Print preview\n",
    "        print(f\"‚úÖ Successfully downloaded 10k (as text) for {ticker}\")\n",
    "        print(\"First 500 characters:\\n\", html_content[:100])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7338282-c1de-4ebf-bd73-8ca644e239ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from openai import OpenAI\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# Create a new OpenAI client with your project key\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "## Select an embeddings model:\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "## Select an LLM model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6689807-79af-4f4b-be23-84ee00386012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10k-text-rag\n",
      "[{\n",
      "    \"name\": \"10k-text-rag\",\n",
      "    \"metric\": \"cosine\",\n",
      "    \"host\": \"10k-text-rag-fqh5rav.svc.aped-4627-b74a.pinecone.io\",\n",
      "    \"spec\": {\n",
      "        \"serverless\": {\n",
      "            \"cloud\": \"aws\",\n",
      "            \"region\": \"us-east-1\"\n",
      "        }\n",
      "    },\n",
      "    \"status\": {\n",
      "        \"ready\": true,\n",
      "        \"state\": \"Ready\"\n",
      "    },\n",
      "    \"vector_type\": \"dense\",\n",
      "    \"dimension\": 3072,\n",
      "    \"deletion_protection\": \"disabled\",\n",
      "    \"tags\": null\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Pinecone setup\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "\n",
    "print(index_name)\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "print(pc.list_indexes())\n",
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "680314a2-4312-47d0-891d-fb8e2d6a6a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create index if it does not exist already\n",
    "import os\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load env\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "pinecone_env = os.getenv(\"PINECONE_ENV\")\n",
    "index_name = os.getenv(\"PINECONE_INDEX_NAME\")\n",
    "load_dotenv()\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)\n",
    "\n",
    "# Delete old index if exists\n",
    "if index_name in pc.list_indexes():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "index_name = \"10k-text-rag\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=\"10k-text-rag\",\n",
    "        dimension=3072,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd905d92-9493-4463-854a-fdb1a2e4f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fbd9804-bf1b-4374-8c56-79ca8feba38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to Qdrant.\n",
      "Qdrant cluster info: title='qdrant - vector search engine' version='1.15.5' commit='48203e414e4e7f639a6d394fb6e4df695f808e51'\n"
     ]
    }
   ],
   "source": [
    "## check connection to QDRANT\n",
    "import numpy as np\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, Range, PointIdsList\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY,\n",
    "    timeout=60,  # Optional: Increase timeout for slow connections or large operations\n",
    ")\n",
    "\n",
    "# Optional: Check connection by listing collections or getting cluster info\n",
    "try:\n",
    "    # Attempt to get cluster info as a basic health check\n",
    "    cluster_info = client.info()\n",
    "    print(\"Successfully connected to Qdrant.\")\n",
    "    print(f\"Qdrant cluster info: {cluster_info}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Qdrant or get cluster info: {e}\")\n",
    "    # Depending on the error, you might want to stop execution here\n",
    "    # raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2de3a52-f7af-4660-a7bf-e2381ee32c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if collection '10k-text-rag' exists...\n",
      "Collection '10k-text-rag' already exists. Deleting it first.\n",
      "Collection '10k-text-rag' deleted. Waiting a moment...\n",
      "Creating collection '10k-text-rag'...\n",
      "Collection '10k-text-rag' created successfully.\n",
      "\n",
      "=== Collection Info                ===\n",
      "\n",
      "status=<CollectionStatus.GREEN: 'green'> optimizer_status=<OptimizersStatusOneOf.OK: 'ok'> warnings=None indexed_vectors_count=0 points_count=0 segments_count=2 config=CollectionConfig(params=CollectionParams(vectors=VectorParams(size=3072, distance=<Distance.COSINE: 'Cosine'>, hnsw_config=None, quantization_config=None, on_disk=None, datatype=None, multivector_config=None), shard_number=1, sharding_method=None, replication_factor=1, write_consistency_factor=1, read_fan_out_factor=None, on_disk_payload=True, sparse_vectors=None), hnsw_config=HnswConfig(m=16, ef_construct=100, full_scan_threshold=10000, max_indexing_threads=0, on_disk=False, payload_m=None, inline_storage=None), optimizer_config=OptimizersConfig(deleted_threshold=0.2, vacuum_min_vector_number=1000, default_segment_number=0, max_segment_size=None, memmap_threshold=None, indexing_threshold=10000, flush_interval_sec=5, max_optimization_threads=None), wal_config=WalConfig(wal_capacity_mb=32, wal_segments_ahead=0, wal_retain_closed=1), quantization_config=None, strict_mode_config=StrictModeConfigOutput(enabled=True, max_query_limit=None, max_timeout=None, unindexed_filtering_retrieve=False, unindexed_filtering_update=False, search_max_hnsw_ef=None, search_allow_exact=None, search_max_oversampling=None, upsert_max_batchsize=None, max_collection_vector_size_bytes=None, read_rate_limit=None, write_rate_limit=None, max_collection_payload_size_bytes=None, max_points_count=None, filter_max_conditions=None, condition_max_size=None, multivector_config=None, sparse_config=None, max_payload_index_count=None), metadata=None) payload_schema={}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"10k-text-rag\"  # Renamed for clarity\n",
    "DIM = 3072\n",
    "fmt = \"\\n=== {:30} ===\\n\"\n",
    "search_latency_fmt = \"search latency = {:.4f}s\"\n",
    "\n",
    "# Ensure the collection doesn't already exist for a clean run\n",
    "try:\n",
    "    print(f\"Checking if collection '{COLLECTION_NAME}' exists...\")\n",
    "    collection_exists = client.collection_exists(collection_name=COLLECTION_NAME)\n",
    "\n",
    "    if collection_exists:\n",
    "        print(f\"Collection '{COLLECTION_NAME}' already exists. Deleting it first.\")\n",
    "        client.delete_collection(collection_name=COLLECTION_NAME)\n",
    "        print(f\"Collection '{COLLECTION_NAME}' deleted. Waiting a moment...\")\n",
    "        time.sleep(2)  # Give Qdrant a moment to process the deletion\n",
    "    else:\n",
    "        print(f\"Collection '{COLLECTION_NAME}' does not exist. Proceeding to create.\")\n",
    "\n",
    "    # Create the collection\n",
    "    print(f\"Creating collection '{COLLECTION_NAME}'...\")\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=DIM, distance=Distance.COSINE),\n",
    "        # Optional: Add optimizers_config, hnsw_config, quantization_config etc. here if needed\n",
    "        # Example: optimizers_config=models.OptimizersConfigDiff(memmap_threshold=20000),\n",
    "        # Example: hnsw_config=models.HnswConfigDiff(m=16, ef_construct=100)\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during collection setup: {e}\")\n",
    "    # raise e\n",
    "\n",
    "# Verify creation by getting collection info\n",
    "try:\n",
    "    collection_info = client.get_collection(collection_name=COLLECTION_NAME)\n",
    "    print(fmt.format(\"Collection Info\"))\n",
    "    print(collection_info)\n",
    "except Exception as e:\n",
    "    print(f\"Error getting collection info for '{COLLECTION_NAME}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1339d52-600c-4537-801c-98a03aa9130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hongn\\AppData\\Local\\Temp\\ipykernel_9504\\1215903231.py:34: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  qdrant.recreate_collection(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created collection: 10k-text-rag\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import PointStruct, VectorParams, Distance\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# -------------------------\n",
    "# 1. Environment variables\n",
    "# -------------------------\n",
    "QDRANT_URL = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY = os.getenv(\"QDRANT_API_KEY\")\n",
    "COLLECTION_NAME = os.getenv(\"QDRANT_COLLECTION_NAME\", \"10k-text-rag\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "EMBED_MODEL = \"text-embedding-3-large\"   # 3072 dimensions\n",
    "\n",
    "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# -------------------------\n",
    "# 2. Connect to Qdrant\n",
    "# -------------------------\n",
    "qdrant = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "# Create or recreate the collection\n",
    "qdrant.recreate_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=3072,               # embedding dimension\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {COLLECTION_NAME}\")\n",
    "\n",
    "# -------------------------\n",
    "# 3. Load and chunk the 10-K text files\n",
    "# -------------------------\n",
    "paths = glob.glob(\"clean_10k_texts/*.txt\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "documents = []\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    chunks = text_splitter.split_text(text)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            \"text\": chunk,\n",
    "            \"source\": os.path.basename(path),\n",
    "            \"chunk\": i\n",
    "        })\n",
    "\n",
    "print(\"Total chunks prepared:\", len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f319dd70-d8df-47ad-b3db-0af0c026338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:   0%|                                                                        | 0/730 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 0, attempt 1: can only concatenate str (not \"int\") to str\n",
      "‚ö†Ô∏è Error uploading batch 0, attempt 2: can only concatenate str (not \"int\") to str\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:   0%|                                                                        | 0/730 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 28\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m         \n\u001b[0;32m     25\u001b[0m         \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;66;03m# 4. OpenAI Embed the batch\u001b[39;00m\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m         response \u001b[38;5;241m=\u001b[39m client_openai\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     29\u001b[0m             model\u001b[38;5;241m=\u001b[39mEMBED_MODEL,\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mbatch_texts\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     32\u001b[0m         embeddings \u001b[38;5;241m=\u001b[39m [item\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;66;03m# 5. Convert to Qdrant points\u001b[39;00m\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 132\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    134\u001b[0m     body\u001b[38;5;241m=\u001b[39mmaybe_transform(params, embedding_create_params\u001b[38;5;241m.\u001b[39mEmbeddingCreateParams),\n\u001b[0;32m    135\u001b[0m     options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[0;32m    136\u001b[0m         extra_headers\u001b[38;5;241m=\u001b[39mextra_headers,\n\u001b[0;32m    137\u001b[0m         extra_query\u001b[38;5;241m=\u001b[39mextra_query,\n\u001b[0;32m    138\u001b[0m         extra_body\u001b[38;5;241m=\u001b[39mextra_body,\n\u001b[0;32m    139\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m    140\u001b[0m         post_parser\u001b[38;5;241m=\u001b[39mparser,\n\u001b[0;32m    141\u001b[0m     ),\n\u001b[0;32m    142\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mCreateEmbeddingResponse,\n\u001b[0;32m    143\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1247\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1254\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1255\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1256\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1257\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1258\u001b[0m     )\n\u001b[1;32m-> 1259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\openai\\_base_client.py:982\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m    980\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 982\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    983\u001b[0m         request,\n\u001b[0;32m    984\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_stream_response_body(request\u001b[38;5;241m=\u001b[39mrequest),\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    986\u001b[0m     )\n\u001b[0;32m    987\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    988\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1011\u001b[0m     )\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    259\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[1;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[1;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(\n\u001b[0;32m    237\u001b[0m         pool_request\u001b[38;5;241m.\u001b[39mrequest\n\u001b[0;32m    238\u001b[0m     )\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    100\u001b[0m     (\n\u001b[0;32m    101\u001b[0m         http_version,\n\u001b[0;32m    102\u001b[0m         status,\n\u001b[0;32m    103\u001b[0m         reason_phrase,\n\u001b[0;32m    104\u001b[0m         headers,\n\u001b[0;32m    105\u001b[0m         trailing_data,\n\u001b[1;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    108\u001b[0m         http_version,\n\u001b[0;32m    109\u001b[0m         status,\n\u001b[0;32m    110\u001b[0m         reason_phrase,\n\u001b[0;32m    111\u001b[0m         headers,\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    219\u001b[0m     )\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1233\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[1;34m(self, buflen, flags)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1230\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1231\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1232\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1233\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(buflen)\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\ssl.py:1106\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[0;32m   1105\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1106\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[0;32m   1108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "point_id = str(uuid.uuid4())  # ALWAYS valid for Qdrant\n",
    "\n",
    "batch_points.append(\n",
    "    PointStruct(\n",
    "        id=point_id,\n",
    "        vector=emb,\n",
    "        payload=doc\n",
    "    )\n",
    ")\n",
    "all_point_ids.append(point_id)\n",
    "\n",
    "for start in tqdm(range(0, len(documents), BATCH_SIZE), desc=\"Uploading chunks\"):\n",
    "    batch_docs = documents[start:start + BATCH_SIZE]\n",
    "    batch_texts = [d[\"text\"] for d in batch_docs]\n",
    "\n",
    "    # Retry logic for embedding + upsert\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            \n",
    "            # -----------------------------\n",
    "            # 4. OpenAI Embed the batch\n",
    "            # -----------------------------\n",
    "            response = client_openai.embeddings.create(\n",
    "                model=EMBED_MODEL,\n",
    "                input=batch_texts\n",
    "            )\n",
    "            embeddings = [item.embedding for item in response.data]\n",
    "\n",
    "            # -----------------------------\n",
    "            # 5. Convert to Qdrant points\n",
    "            # -----------------------------\n",
    "            batch_points = []\n",
    "\n",
    "            for doc, emb in zip(batch_docs, embeddings):\n",
    "                batch_points.append(\n",
    "                    PointStruct(\n",
    "                        id=str(point_id),\n",
    "                        vector=emb,\n",
    "                        payload=doc\n",
    "                    )\n",
    "                )\n",
    "                all_point_ids.append(point_id)\n",
    "                point_id += 1\n",
    "\n",
    "            # -----------------------------\n",
    "            # 6. Upsert into Qdrant\n",
    "            # -----------------------------\n",
    "            qdrant.upsert(\n",
    "                collection_name=COLLECTION_NAME,\n",
    "                points=batch_points\n",
    "            )\n",
    "\n",
    "            break  # success ‚Üí exit retry loop\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error uploading batch {start//BATCH_SIZE}, attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "\n",
    "print(\"Done!\")\n",
    "print(f\"Total vectors inserted: {len(all_point_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dda1a84-a6c8-497d-967a-3485c4e77514",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found for ticker 'ABBV' at path: company_details/EDGAR (US)/filings/ABBV_10K.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m ticker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mABBV\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# or pass dynamically\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m text_content \u001b[38;5;241m=\u001b[39m download_text_from_storage(ticker)\n\u001b[0;32m      7\u001b[0m output_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/hongn/idealy_new/idealy/backend/rag-api/clean_10k_texts\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save cleaned text\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 58\u001b[0m, in \u001b[0;36mdownload_text_from_storage\u001b[1;34m(ticker, service_account_path)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Check if the text file exists\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found for ticker \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mticker\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Download the text content\u001b[39;00m\n\u001b[0;32m     61\u001b[0m text_content \u001b[38;5;241m=\u001b[39m blob\u001b[38;5;241m.\u001b[39mdownload_as_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File not found for ticker 'ABBV' at path: company_details/EDGAR (US)/filings/ABBV_10K.txt"
     ]
    }
   ],
   "source": [
    "## Convert HTML to text file and store\n",
    "import os\n",
    "\n",
    "ticker = \"ABBV\"  # or pass dynamically\n",
    "text_content = download_text_from_storage(ticker)\n",
    "\n",
    "output_folder = \"C:/Users/hongn/idealy_new/idealy/backend/rag-api/clean_10k_texts\"\n",
    "\n",
    "# Save cleaned text\n",
    "output_path = os.path.join(output_folder, f\"{ticker}_10K.txt\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "print(f\"‚úÖ Saved cleaned text to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f356dfe1-42ae-4a6a-a88d-d5a68119b94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 124 documents.\n",
      "\n",
      "First document metadata: {'source': 'clean_10k_texts\\\\AAPL_2025_10K.txt'}\n",
      "\n",
      "--- First 500 characters ---\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "UNITED STATES\n",
      "\n",
      "SECURITIES AND EXCHANGE COMMISSION\n",
      "\n",
      "Washington, D.C. 20549\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "FORM 10-K\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "(Mark One)\n",
      "\n",
      "‚òí ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "For the fiscal year ended September 27, 2025\n",
      "\n",
      "or\n",
      "\n",
      "‚òê TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
      "\n",
      "For the transition period from  to  .\n",
      "\n",
      "Commission File Number: 001-36743\n",
      "\n",
      "| |   \n",
      "---|---|---  \n",
      "\n",
      "Apple Inc.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "\n",
    "# Path to the folder containing your cleaned text files\n",
    "TEXT_FOLDER = \"./clean_10k_texts\"\n",
    "\n",
    "# Load all .txt files\n",
    "loader = DirectoryLoader(\n",
    "    TEXT_FOLDER,\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(docs)} documents.\\n\")\n",
    "\n",
    "# Preview first document\n",
    "print(\"First document metadata:\", docs[0].metadata)\n",
    "print(\"\\n--- First 500 characters ---\\n\")\n",
    "print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27664e65-0d03-48c7-991f-75d619411ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1 document with 279148 characters.\n",
      "| |   \n",
      "---|---|---  \n",
      "UNITED STATES SECURITIES AND EXCHANGE COMMISSION\n",
      "\n",
      "Washington, D.C. 20549\n",
      "\n",
      "_____________________________________________________________________\n",
      "\n",
      "FORM 10-K\n",
      "\n",
      "_____________________________________________________________________\n",
      "\n",
      "(Mark One)\n",
      "\n",
      "| | | | |   \n",
      "---|---|---|---|---|---  \n",
      "‚òí| ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934  \n",
      "\n",
      "For the fiscal year ended December 31, 2024\n",
      "\n",
      "OR\n",
      "\n",
      "| | | | |   \n",
      "---|---|---|---|---|---  \n",
      "‚òê| TRANSITION REPORT P\n"
     ]
    }
   ],
   "source": [
    "## Load documents\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# Path to the folder containing your cleaned text files\n",
    "TEXT_FOLDER = \"./clean_10k_texts\"  # e.g., output of clean_10k_html()\n",
    "\n",
    "# Option 1: Load a single file\n",
    "single_file_path = os.path.join(TEXT_FOLDER, \"NFLX_10K.txt\")\n",
    "loader = TextLoader(single_file_path, encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"‚úÖ Loaded 1 document with {len(docs[0].page_content)} characters.\")\n",
    "print(docs[0].page_content[:500])\n",
    "\n",
    "# Option 2: Load all text files in a folder\n",
    "# loader = DirectoryLoader(TEXT_FOLDER, glob=\"*.txt\", loader_cls=TextLoader, loader_kwargs={\"encoding\": \"utf-8\"})\n",
    "# docs = loader.load()\n",
    "\n",
    "# print(f\"‚úÖ Loaded {len(docs)} documents.\")\n",
    "# print(docs[0].metadata)\n",
    "# print(docs[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "292a8949-c436-495b-b48c-872cdf9f35c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split into 93427 chunks.\n"
     ]
    }
   ],
   "source": [
    "## Docs to Chunks via textsplitter functions\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, \n",
    "    add_start_index=True)\n",
    "\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "print(f\"‚úÖ Split into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "656a42a9-6a41-4bec-9ee3-29164ba74317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                    | 314/468 [25:49<12:17,  4.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 314, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:47 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '96', 'x-pinecone-request-id': '7982488119216157449', 'x-envoy-upstream-service-time': '28', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 314, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '66', 'x-pinecone-request-id': '1847917596060236855', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 314, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:46:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '79', 'x-pinecone-request-id': '7812485040887677095', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 315/468 [26:07<22:43,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 315, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:03 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '99', 'x-pinecone-request-id': '6887225196364178439', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 315, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:09 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '163', 'x-pinecone-request-id': '1152302126835574217', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 315, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:16 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '67', 'x-pinecone-request-id': '5751893401277074809', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                    | 316/468 [26:26<29:52, 11.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 316, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:22 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '74', 'x-pinecone-request-id': '3443433195536649323', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 316, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '81', 'x-pinecone-request-id': '4455966097111190929', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 316, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:36 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '119', 'x-pinecone-request-id': '4187682213216210893', 'x-envoy-upstream-service-time': '9', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                    | 317/468 [26:46<36:27, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 317, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:43 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '79', 'x-pinecone-request-id': '5074216660346096774', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 317, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:49 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '85', 'x-pinecone-request-id': '4451538800431435109', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 317, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:47:57 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '72', 'x-pinecone-request-id': '8232850220250487554', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                   | 318/468 [27:06<40:20, 16.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 318, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:04 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '103', 'x-pinecone-request-id': '4980604788259433491', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 318, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:11 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '64', 'x-pinecone-request-id': '3450590172140288414', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 318, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:17 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '62', 'x-pinecone-request-id': '1277798384400842462', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                   | 319/468 [27:27<43:21, 17.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 319, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:23 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '107', 'x-pinecone-request-id': '5810758915277509526', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 319, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:29 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '110', 'x-pinecone-request-id': '8702142190872492001', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 319, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:35 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '98', 'x-pinecone-request-id': '796670792108520295', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                   | 320/468 [27:45<43:39, 17.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 320, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:42 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '141', 'x-pinecone-request-id': '2918533118264848584', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 320, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:48 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '115', 'x-pinecone-request-id': '217203292585814881', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 320, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:48:54 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '70', 'x-pinecone-request-id': '2654920316656810352', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                   | 321/468 [28:04<44:09, 18.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 321, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:01 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '95', 'x-pinecone-request-id': '2395756261778790895', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 321, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:07 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '105', 'x-pinecone-request-id': '5033429704561069676', 'x-envoy-upstream-service-time': '3', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 321, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '76', 'x-pinecone-request-id': '3084926058212948187', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                   | 322/468 [28:23<45:00, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 322, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:20 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '63', 'x-pinecone-request-id': '7884871746501029296', 'x-envoy-upstream-service-time': '7', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 322, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:26 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '59', 'x-pinecone-request-id': '4345499180109554876', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 322, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '101', 'x-pinecone-request-id': '4091295665732925367', 'x-envoy-upstream-service-time': '9', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                   | 323/468 [28:43<45:12, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 323, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:40 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '113', 'x-pinecone-request-id': '1783150408390401669', 'x-envoy-upstream-service-time': '7', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 323, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:46 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '91', 'x-pinecone-request-id': '957619681063049265', 'x-envoy-upstream-service-time': '8', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 323, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:52 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '64', 'x-pinecone-request-id': '1648590854806108805', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                   | 324/468 [29:02<45:02, 18.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 324, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:49:58 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '128', 'x-pinecone-request-id': '7256124030153667348', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 324, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:05 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '70', 'x-pinecone-request-id': '7280234316967944236', 'x-envoy-upstream-service-time': '4', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 324, attempt 3: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:10 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '114', 'x-pinecone-request-id': '6012754374071021034', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 325/468 [29:20<44:46, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Error uploading batch 325, attempt 1: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '110', 'x-pinecone-request-id': '7934570794545337204', 'x-envoy-upstream-service-time': '6', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n",
      "‚ö†Ô∏è Error uploading batch 325, attempt 2: (429)\n",
      "Reason: Too Many Requests\n",
      "HTTP response headers: HTTPHeaderDict({'Date': 'Mon, 17 Nov 2025 18:50:24 GMT', 'Content-Type': 'application/json', 'Content-Length': '166', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '92', 'x-pinecone-request-id': '6497272980675131265', 'x-envoy-upstream-service-time': '5', 'server': 'envoy'})\n",
      "HTTP response body: {\"code\":8,\"message\":\"Request failed. You've reached your write unit limit for the current month (2000000). To continue writing data, upgrade your plan.\",\"details\":[]}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading chunks:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                   | 325/468 [29:37<13:02,  5.47s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m         ids \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[0;32m     14\u001b[0m         document_ids\u001b[38;5;241m.\u001b[39mextend(ids)\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# break retry loop\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:258\u001b[0m, in \u001b[0;36mVectorStore.add_documents\u001b[1;34m(self, documents, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m    257\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    259\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m )\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\langchain_pinecone\\vectorstores.py:362\u001b[0m, in \u001b[0;36mPineconeVectorStore.add_texts\u001b[1;34m(self, texts, metadatas, ids, namespace, batch_size, embedding_chunk_size, async_req, id_prefix, **kwargs)\u001b[0m\n\u001b[0;32m    358\u001b[0m vector_tuples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(chunk_ids, embeddings, chunk_metadatas))\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m async_req:\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# Runs the pinecone upsert asynchronously.\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     async_res \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 362\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mupsert(\n\u001b[0;32m    363\u001b[0m             vectors\u001b[38;5;241m=\u001b[39mbatch_vector_tuples,\n\u001b[0;32m    364\u001b[0m             namespace\u001b[38;5;241m=\u001b[39mnamespace,\n\u001b[0;32m    365\u001b[0m             async_req\u001b[38;5;241m=\u001b[39masync_req,\n\u001b[0;32m    366\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    367\u001b[0m         )\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_vector_tuples \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, vector_tuples)\n\u001b[0;32m    369\u001b[0m     ]\n\u001b[0;32m    370\u001b[0m     [res\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m async_res]\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\utils\\error_handling.py:15\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_func\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     17\u001b[0m         \u001b[38;5;66;03m# Lazy import of urllib3 exceptions\u001b[39;00m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MaxRetryError, ProtocolError \u001b[38;5;28;01mas\u001b[39;00m Urllib3ProtocolError\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\index.py:212\u001b[0m, in \u001b[0;36mIndex.upsert\u001b[1;34m(self, vectors, namespace, batch_size, show_progress, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masync_req is not supported when batch_size is provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo upsert in parallel, please follow: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.pinecone.io/docs/insert-data#sending-upserts-in-parallel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m     )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsert_batch(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch_size, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m batch_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\index.py:239\u001b[0m, in \u001b[0;36mIndex._upsert_batch\u001b[1;34m(self, vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_upsert_batch\u001b[39m(\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    231\u001b[0m     vectors: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    237\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m UpsertResponse:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_vector_api\u001b[38;5;241m.\u001b[39mupsert_vectors(\n\u001b[1;32m--> 239\u001b[0m         IndexRequestFactory\u001b[38;5;241m.\u001b[39mupsert_request(vectors, namespace, _check_type, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_openapi_kwargs(kwargs),\n\u001b[0;32m    241\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\request_factory.py:93\u001b[0m, in \u001b[0;36mIndexRequestFactory.upsert_request\u001b[1;34m(vectors, namespace, _check_type, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvec_builder\u001b[39m(v):\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m UpsertRequest(\n\u001b[1;32m---> 93\u001b[0m     vectors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(vec_builder, vectors)),\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs_dict,\n\u001b[0;32m     95\u001b[0m     _check_type\u001b[38;5;241m=\u001b[39m_check_type,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnon_openapi_kwargs(kwargs),\n\u001b[0;32m     97\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\request_factory.py:90\u001b[0m, in \u001b[0;36mIndexRequestFactory.upsert_request.<locals>.vec_builder\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvec_builder\u001b[39m(v):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39mbuild(v, check_type\u001b[38;5;241m=\u001b[39m_check_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\vector_factory.py:48\u001b[0m, in \u001b[0;36mVectorFactory.build\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenApiVector(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_tuple_to_vector(item, check_type)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Mapping):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VectorFactory\u001b[38;5;241m.\u001b[39m_dict_to_vector(item, check_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\db_data\\vector_factory.py:64\u001b[0m, in \u001b[0;36mVectorFactory._tuple_to_vector\u001b[1;34m(item, check_type)\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparse values are not supported in tuples. Please use either dicts or OpenApiVector objects as inputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OpenApiVector(\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mid\u001b[39m,\n\u001b[0;32m     66\u001b[0m         values\u001b[38;5;241m=\u001b[39mconvert_to_list(values),\n\u001b[0;32m     67\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[0;32m     68\u001b[0m         _check_type\u001b[38;5;241m=\u001b[39mcheck_type,\n\u001b[0;32m     69\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:36\u001b[0m, in \u001b[0;36mconvert_js_args_to_python_args.<locals>.wrapped_init\u001b[1;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec_property_naming:\n\u001b[0;32m     33\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m change_keys_js_to_python(\n\u001b[0;32m     34\u001b[0m         kwargs, _self \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_self, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _self\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m     35\u001b[0m     )\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(_self, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\core\\openapi\\db_data\\model\\vector.py:294\u001b[0m, in \u001b[0;36mVector.__init__\u001b[1;34m(self, id, *args, **kwargs)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    287\u001b[0m     var_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_map\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configuration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m ):\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;66;03m# discard variable.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 294\u001b[0m \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_name, var_value)\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_only_vars:\n\u001b[0;32m    296\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiAttributeError(\n\u001b[0;32m    297\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` is a read-only attribute. Use `from_openapi_data` to instantiate \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass with read only attributes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    299\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:171\u001b[0m, in \u001b[0;36mOpenApiModel.__setattr__\u001b[1;34m(self, attr, value)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, value):\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"set the value of an attribute using dot notation: `instance.attr = val`\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m[attr] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:456\u001b[0m, in \u001b[0;36mModelNormal.__setitem__\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[name] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_attribute(name, value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:139\u001b[0m, in \u001b[0;36mOpenApiModel.set_attribute\u001b[1;34m(self, name, value)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiTypeError(\n\u001b[0;32m    135\u001b[0m         error_msg, path_to_item\u001b[38;5;241m=\u001b[39mpath_to_item, valid_classes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mstr\u001b[39m,), key_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    136\u001b[0m     )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_type:\n\u001b[1;32m--> 139\u001b[0m     value \u001b[38;5;241m=\u001b[39m validate_and_convert_types(\n\u001b[0;32m    140\u001b[0m         value,\n\u001b[0;32m    141\u001b[0m         required_types_mixed,\n\u001b[0;32m    142\u001b[0m         path_to_item,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec_property_naming,\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_type,\n\u001b[0;32m    145\u001b[0m         configuration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configuration,\n\u001b[0;32m    146\u001b[0m     )\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (name,) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_values \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_allowed_values:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;66;03m# Disabling allowed_value validation on response makes the SDK\u001b[39;00m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;66;03m# less fragile if unexpected values are returned. For example, if\u001b[39;00m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;66;03m# an unexpected index status is returned, we don't want to break\u001b[39;00m\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;66;03m# when listing indexes due to validation on the status field against\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;66;03m# the allowed values in the enum.\u001b[39;00m\n\u001b[0;32m    153\u001b[0m     check_allowed_values(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_values, (name,), value)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pinecone\\openapi_support\\model_utils.py:1548\u001b[0m, in \u001b[0;36mvalidate_and_convert_types\u001b[1;34m(input_value, required_types_mixed, path_to_item, spec_property_naming, _check_type, configuration)\u001b[0m\n\u001b[0;32m   1546\u001b[0m         inner_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(path_to_item)\n\u001b[0;32m   1547\u001b[0m         inner_path\u001b[38;5;241m.\u001b[39mappend(index)\n\u001b[1;32m-> 1548\u001b[0m         input_value[index] \u001b[38;5;241m=\u001b[39m validate_and_convert_types(\n\u001b[0;32m   1549\u001b[0m             inner_value,\n\u001b[0;32m   1550\u001b[0m             inner_required_types,\n\u001b[0;32m   1551\u001b[0m             inner_path,\n\u001b[0;32m   1552\u001b[0m             spec_property_naming,\n\u001b[0;32m   1553\u001b[0m             _check_type,\n\u001b[0;32m   1554\u001b[0m             configuration\u001b[38;5;241m=\u001b[39mconfiguration,\n\u001b[0;32m   1555\u001b[0m         )\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(input_value, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_value \u001b[38;5;241m==\u001b[39m {}:\n\u001b[0;32m   1558\u001b[0m         \u001b[38;5;66;03m# allow an empty dict\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## load chunks into the Pinecone Vector store ('vector_store' defined earlier)\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "BATCH_SIZE = 200  # You can tune this (100‚Äì300 is ideal)\n",
    "document_ids = []\n",
    "\n",
    "for i in tqdm(range(0, len(chunks), BATCH_SIZE), desc=\"Uploading chunks\"):\n",
    "    batch = chunks[i:i + BATCH_SIZE]\n",
    "\n",
    "    # Retry logic (recommended for Pinecone)\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            ids = vector_store.add_documents(documents=batch)\n",
    "            document_ids.extend(ids)\n",
    "            break  # break retry loop\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error uploading batch {i//BATCH_SIZE}, attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)  # wait + retry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e736858-7839-4eda-8e92-26d6245aa8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(document_ids))\n",
    "vector_store.similarity_search(\"What does Apple say about revenue recognition?\", k=2)\n",
    "# the above returns 2 relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "447d8f8a-1b6a-4863-82e3-456c95aca04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2765ea43-32e4-4ea6-a6fb-08b4c06d3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Example using local API key or .env\n",
    "llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb6cb450-db22-4f96-bfc6-2673714d44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_context]\n",
    "\n",
    "# System prompt / instructions for the agent\n",
    "system_prompt = (\n",
    "    \"You are a helpful assistant that answers user queries using the provided context. \"\n",
    "    \"Use the retrieval tool when necessary.\"\n",
    "    \"provide succinct answers and as quantitative answers as available from retrieval\"\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_agent(llm, tools=tools, system_prompt=system_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c74d1e2-cc6e-42b2-ac64-37c6cb58d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Morgan Stanley makes money primarily through the following business segments:\n",
      "\n",
      "1. Investment Banking: Revenue from financial advisory and underwriting assignments.\n",
      "2. Investment Management: Revenue from providing asset management and wealth advisory services.\n",
      "3. Commissions and Fees: Revenue from executing and clearing client transactions on stock, options, and futures exchanges, as well as over-the-counter (OTC) transactions.\n",
      "4. Markets: Revenue from trading activities, including equity and fixed income markets.\n",
      "\n",
      "Products and Services & Revenue (latest available):\n",
      "- Markets (trading, including equities and fixed income): $19.8 billion in revenue.\n",
      "  - Equity Markets: Growth driven by cash equities, equity derivatives, and prime services.\n",
      "  - Fixed Income Markets: Growth in asset-backed financing, securitization, and underwriting fees.\n",
      "- Investment Banking and Investment Management: Specific revenue figures for these segments are not provided in the retrieved context, but they are major contributors.\n",
      "\n",
      "Main Competitors:\n",
      "Morgan Stanley‚Äôs main competitors include:\n",
      "- Goldman Sachs\n",
      "- JPMorgan Chase\n",
      "- Bank of America Merrill Lynch\n",
      "- Citigroup\n",
      "- Wells Fargo\n",
      "- Other global and regional investment banks, asset managers, and financial technology firms.\n",
      "\n",
      "Let me know if you need more detailed revenue breakdowns or information on a specific product or service.\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"How does Morgan Stanley make money??\\n\\n\"\n",
    "    \"Once you get the answer, please find some of the products and services they sell and how much money each product made.\\n\\n\"\n",
    "    \"Who are Morgan Stanley's main competitors?\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    msg = event[\"messages\"][-1]\n",
    "\n",
    "    # Only print the final AI messages\n",
    "    if getattr(msg, \"type\", None) == \"ai\": # do not get human, tool messages, only the AI response\n",
    "        print(msg.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5cb434-c32c-47df-ad1a-f804b4226b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
