{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "039e235a-3675-4716-bbd0-1eadca8a5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "BASE = \"https://data.sec.gov\"\n",
    "HEADERS = {\"User-Agent\": \"kurio-agent/1.0 (potatojacket9@gmail.com)\"}\n",
    "\n",
    "## Step 2. Get CIK for the input ticker symbol~\n",
    "def get_cik(ticker):\n",
    "    \"\"\"Retrieve CIK for a given ticker symbol\"\"\"\n",
    "    res = requests.get(f\"{BASE}/submissions/CIK{ticker}.json\", headers=HEADERS)\n",
    "    if res.status_code != 200:\n",
    "        # fallback: try SEC ticker endpoint\n",
    "        lookup = requests.get(f\"https://www.sec.gov/files/company_tickers.json\", headers=HEADERS).json()\n",
    "        for _, c in lookup.items():\n",
    "            if c[\"ticker\"].lower() == ticker.lower():\n",
    "                return str(c[\"cik_str\"]).zfill(10)\n",
    "    return None\n",
    "\n",
    "## Step 3. Get latest 10K url for the CIK (ticker)\n",
    "def get_latest_10k_url(cik):\n",
    "    \"\"\"Retrieve latest 10-K filing document URL and dates for a given CIK\"\"\"\n",
    "    url = f\"{BASE}/submissions/CIK{cik}.json\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    data = res.json()\n",
    "\n",
    "    for form, acc, filing_date, report_date in zip(\n",
    "        data[\"filings\"][\"recent\"][\"form\"],\n",
    "        data[\"filings\"][\"recent\"][\"accessionNumber\"],\n",
    "        data[\"filings\"][\"recent\"][\"filingDate\"],\n",
    "        data[\"filings\"][\"recent\"][\"reportDate\"]\n",
    "    ):\n",
    "        if form == \"10-K\":\n",
    "            acc_num = acc.replace(\"-\", \"\")\n",
    "            filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_num}/{acc}-index.html\"\n",
    "            return filing_url, filing_date, report_date\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "## Step 3a. Define ability to find to extract cashflow tags from XML soup (input is soup). This is not dependent on any function\n",
    "def parse_cashflow_from_xbrl(soup):\n",
    "    tags = {\n",
    "        # Operating\n",
    "        \"Net profit (or loss if negative)\": \"us-gaap:NetIncomeLoss\",\n",
    "        \"Depreciation (wear & tear on assets)\": \"us-gaap:DepreciationDepletionAndAmortization\",\n",
    "        \"stock_comp\": \"us-gaap:ShareBasedCompensation\",\n",
    "        \"change_ar\": \"us-gaap:IncreaseDecreaseInAccountsReceivable\",\n",
    "        \"change_inventory\": \"us-gaap:IncreaseDecreaseInInventory\",\n",
    "        \"change_ap\": \"us-gaap:IncreaseDecreaseInAccountsPayable\",\n",
    "        \"Cash from day-to-day business (Operating Cashflow)\": \"us-gaap:NetCashProvidedByUsedInOperatingActivities\",\n",
    "\n",
    "        # Investing\n",
    "        \"Buying equipment/buildings (Capital Expenditure)\": \"us-gaap:PaymentsToAcquirePropertyPlantAndEquipment\",\n",
    "        \"acquisitions\": \"us-gaap:PaymentsToAcquireBusinessesNetOfCashAcquired\",\n",
    "        \"asset_sales\": \"us-gaap:ProceedsFromSaleOfPropertyPlantAndEquipment\",\n",
    "        \"investments_purchase\": \"us-gaap:PaymentsToAcquireMarketableSecurities\",\n",
    "        \"investments_maturity\": \"us-gaap:ProceedsFromMaturitiesOfMarketableSecurities\",\n",
    "        \"Cash from investments (Buying/Selling assets)\": \"us-gaap:NetCashProvidedByUsedInInvestingActivities\",\n",
    "\n",
    "        # Financing\n",
    "        \"Money raised from issuing new shares\": \"us-gaap:ProceedsFromIssuanceOfCommonStock\",\n",
    "        \"Money spent buying back shares of company\": \"us-gaap:PaymentsForRepurchaseOfCommonStock\",\n",
    "        \"Borrowed money (New loans or bonds)\": \"us-gaap:ProceedsFromIssuanceOfLongTermDebt\",\n",
    "        \"Loan repayments\": \"us-gaap:RepaymentsOfLongTermDebt\",\n",
    "        \"Dividends paid to shareholders\": \"us-gaap:PaymentsOfDividends\",\n",
    "        \"Cash from investors and loans (Financing activities)\": \"us-gaap:NetCashProvidedByUsedInFinancingActivities\",\n",
    "\n",
    "        # Summary\n",
    "        \"Change in cash during the period\": \"us-gaap:CashAndCashEquivalentsPeriodIncreaseDecrease\",\n",
    "        \"Cash at the beginning of the period\": \"us-gaap:CashAndCashEquivalentsAtBeginningOfPeriod\",\n",
    "        \"Cash remaining at the end of the period\": \"us-gaap:CashAndCashEquivalentsAtCarryingValue\",\n",
    "    }\n",
    "\n",
    "    data = {}\n",
    "    for key, tag in tags.items():\n",
    "        el = soup.find(tag)\n",
    "        if el and hasattr(el, \"text\"):\n",
    "            try:\n",
    "                data[key] = float(el.text.strip().replace(\",\", \"\"))\n",
    "            except ValueError:\n",
    "                data[key] = el.text.strip()\n",
    "        else:\n",
    "            data[key] = None\n",
    "\n",
    "    return data\n",
    "\n",
    "## Step 3b. Define ability to find to extract cashflow tags from XML soup (input is soup). This is not dependent on any function\n",
    "def parse_income_from_xbrl(soup):\n",
    "    \"\"\"\n",
    "    Extract key income statement values from an XBRL soup object and group\n",
    "    them into: revenue, expenses, profit, and shares.\n",
    "    If some values are missing, compute derived ones (e.g. Gross Profit = Revenue - Cost of Revenue).\n",
    "    \"\"\"\n",
    "\n",
    "    # Define XBRL tags grouped by category\n",
    "    tags = {\n",
    "        \"revenue\": {\n",
    "            \"Total Revenue\": [\n",
    "                \"us-gaap:Revenues\",\n",
    "                \"us-gaap:RevenueFromContractWithCustomerExcludingAssessedTax\",\n",
    "                \"us-gaap:SalesRevenueNet\",\n",
    "                \"us-gaap:SalesRevenueGoodsNet\",\n",
    "                \"us-gaap:SalesRevenueServicesNet\"\n",
    "            ],\n",
    "            \"Advertising Revenue\": [\"us-gaap:AdvertisingRevenue\"],\n",
    "            \"Interest Income\": [\"us-gaap:InterestIncome\"],\n",
    "            \"Other Income\": [\"us-gaap:OtherNonoperatingIncomeExpense\"]\n",
    "        },\n",
    "\n",
    "        \"expenses\": {\n",
    "            \"Cost of Revenue\": [\"us-gaap:CostOfRevenue\", \"us-gaap:CostOfGoodsSold\"],\n",
    "            \"Research & Development\": [\"us-gaap:ResearchAndDevelopmentExpense\"],\n",
    "            \"Sales & Marketing\": [\"us-gaap:SellingAndMarketingExpense\"],\n",
    "            \"General & Administrative\": [\"us-gaap:GeneralAndAdministrativeExpense\"],\n",
    "            \"Operating Expenses (Total)\": [\"us-gaap:OperatingExpenses\"],\n",
    "            \"Interest Expense\": [\"us-gaap:InterestExpense\"],\n",
    "            \"Income Tax Expense\": [\"us-gaap:IncomeTaxExpenseBenefit\"]\n",
    "        },\n",
    "\n",
    "        \"profit\": {\n",
    "            \"Gross Profit\": [\"us-gaap:GrossProfit\"],\n",
    "            \"Operating Income\": [\"us-gaap:OperatingIncomeLoss\"],\n",
    "            \"Income Before Tax\": [\n",
    "                \"us-gaap:IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest\"\n",
    "            ],\n",
    "            \"Net Income\": [\"us-gaap:NetIncomeLoss\"]\n",
    "        },\n",
    "\n",
    "        \"shares\": {\n",
    "            \"Earnings per Share (Basic)\": [\"us-gaap:EarningsPerShareBasic\"],\n",
    "            \"Earnings per Share (Diluted)\": [\"us-gaap:EarningsPerShareDiluted\"],\n",
    "            \"Weighted Average Shares Outstanding (Basic)\": [\n",
    "                \"us-gaap:WeightedAverageNumberOfSharesOutstandingBasic\"\n",
    "            ],\n",
    "            \"Weighted Average Shares Outstanding (Diluted)\": [\n",
    "                \"us-gaap:WeightedAverageNumberOfDilutedSharesOutstanding\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Helper function to extract numeric value from XBRL\n",
    "    def extract_value(tag_list):\n",
    "        for tag in tag_list:\n",
    "            el = soup.find(tag)\n",
    "            if el and el.text.strip():\n",
    "                try:\n",
    "                    return float(el.text.strip().replace(\",\", \"\"))\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        return None\n",
    "\n",
    "    # Parse raw values from XBRL\n",
    "    grouped_data = {}\n",
    "    for section, section_tags in tags.items():\n",
    "        grouped_data[section] = {}\n",
    "        for label, tag_list in section_tags.items():\n",
    "            grouped_data[section][label] = extract_value(tag_list)\n",
    "\n",
    "    # --- üßÆ Compute derived metrics ---\n",
    "    rev = grouped_data[\"revenue\"].get(\"Total Revenue\")\n",
    "    cost = grouped_data[\"expenses\"].get(\"Cost of Revenue\")\n",
    "    gross = grouped_data[\"profit\"].get(\"Gross Profit\")\n",
    "    op_exp = grouped_data[\"expenses\"].get(\"Operating Expenses (Total)\")\n",
    "    r_and_d = grouped_data[\"expenses\"].get(\"Research & Development\")\n",
    "    s_and_m = grouped_data[\"expenses\"].get(\"Sales & Marketing\")\n",
    "    g_and_a = grouped_data[\"expenses\"].get(\"General & Administrative\")\n",
    "    op_inc = grouped_data[\"profit\"].get(\"Operating Income\")\n",
    "\n",
    "    # Gross Profit = Revenue - Cost of Revenue\n",
    "    if gross is None and rev is not None and cost is not None:\n",
    "        grouped_data[\"profit\"][\"Gross Profit\"] = rev - cost\n",
    "\n",
    "    # Operating Expenses (Total) = R&D + Sales & Marketing + G&A\n",
    "    if op_exp is None and any(v is not None for v in [r_and_d, s_and_m, g_and_a]):\n",
    "        total = sum(v for v in [r_and_d, s_and_m, g_and_a] if v is not None)\n",
    "        grouped_data[\"expenses\"][\"Operating Expenses (Total)\"] = total\n",
    "\n",
    "    # Operating Income = Gross Profit - Operating Expenses\n",
    "    gross = grouped_data[\"profit\"].get(\"Gross Profit\")\n",
    "    op_exp = grouped_data[\"expenses\"].get(\"Operating Expenses (Total)\")\n",
    "    if op_inc is None and gross is not None and op_exp is not None:\n",
    "        grouped_data[\"profit\"][\"Operating Income\"] = gross - op_exp\n",
    "\n",
    "    # Income Before Tax = Operating Income + (Interest Income - Interest Expense) + Other Income\n",
    "    inc_before_tax = grouped_data[\"profit\"].get(\"Income Before Tax\")\n",
    "    int_income = grouped_data[\"revenue\"].get(\"Interest Income\")\n",
    "    int_exp = grouped_data[\"expenses\"].get(\"Interest Expense\")\n",
    "    other_inc = grouped_data[\"revenue\"].get(\"Other Income\")\n",
    "    if inc_before_tax is None and op_inc is not None:\n",
    "        total_other = (int_income or 0) - (int_exp or 0) + (other_inc or 0)\n",
    "        grouped_data[\"profit\"][\"Income Before Tax\"] = op_inc + total_other\n",
    "\n",
    "    return grouped_data\n",
    "\n",
    "## Step 4: Find actual XBRL XML file from 10K URL\n",
    "def get_primary_xbrl_url(index_url):\n",
    "    \"\"\"Find the main XBRL (XML) file inside the 10-K index page\"\"\"\n",
    "    res = requests.get(index_url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"]\n",
    "        if href.endswith(\".xml\") and not any(\n",
    "            x in href for x in [\"_cal.xml\", \"_lab.xml\", \"_pre.xml\", \"_def.xml\"]\n",
    "        ):\n",
    "            return \"https://www.sec.gov\" + href\n",
    "\n",
    "    return None\n",
    "\n",
    "## Step 5. Parse a given filing (the actual XBRL or XML)\n",
    "def parse_cashflow_from_10k(latest_10k_filing_XBRL_XML):\n",
    "    \"\"\"Extract cash flow data from a 10-K XBRL or XML filing\"\"\"\n",
    "    response = requests.get(latest_10k_filing_XBRL_XML, headers=HEADERS)\n",
    "\n",
    "    # Detect whether the document is XML or HTML\n",
    "    parser = \"xml\" if \"<?xml\" in response.text[:100] else \"html.parser\"\n",
    "    soup = BeautifulSoup(response.text, features=parser)\n",
    "\n",
    "    # Extract cashflow data using your earlier helper\n",
    "    output = parse_cashflow_from_xbrl(soup)\n",
    "\n",
    "    return output  # ‚úÖ actually return the data\n",
    "\n",
    "## Step 6. Parse a given filing to get the income statement\n",
    "def parse_income_from_10k(latest_10k_filing_XBRL_XML):\n",
    "    \"\"\"Extract income statement data from a 10-K XBRL or XML filing\"\"\"\n",
    "    response = requests.get(latest_10k_filing_XBRL_XML, headers=HEADERS)\n",
    "\n",
    "    # Detect whether the document is XML or HTML\n",
    "    parser = \"xml\" if \"<?xml\" in response.text[:100] else \"html.parser\"\n",
    "    soup = BeautifulSoup(response.text, features=parser)\n",
    "\n",
    "    # Extract income data using your helper function\n",
    "    output = parse_income_from_xbrl(soup)\n",
    "\n",
    "    return output  # ‚úÖ return the parsed income statement data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f985c22-6807-467d-b751-537097cf57ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cashflow_json(ticker):\n",
    "    \"\"\"End-to-end: from ticker ‚Üí CIK ‚Üí 10-K ‚Üí XBRL ‚Üí cashflow data\"\"\"\n",
    "    ## Step 2. Get CIK for the input ticker symbol\n",
    "    cik = get_cik(ticker)\n",
    "    if not cik:\n",
    "        raise ValueError(\"CIK not found.\")\n",
    "\n",
    "    ## Step 3. Get latest 10K url for the CIK (ticker)\n",
    "    filing_url, filing_date, report_date = get_latest_10k_url(cik)\n",
    "    if not filing_url:\n",
    "        raise ValueError(\"No 10-K filing found.\")\n",
    "\n",
    "    print(f\"Found 10-K filing: {filing_url} (filed {filing_date}, for {report_date})\")\n",
    "\n",
    "    # Step 4: Find actual XBRL XML file from 10K URL\n",
    "    xbrl_url = get_primary_xbrl_url(filing_url)\n",
    "    if not xbrl_url:\n",
    "        raise ValueError(\"No XBRL XML file found in filing.\")\n",
    "\n",
    "    print(f\"Using XBRL XML file: {xbrl_url}\")\n",
    "\n",
    "\n",
    "    # Step 5: Parse cashflow data from that XML\n",
    "    cashflow_data = parse_cashflow_from_10k(xbrl_url)\n",
    "    if not cashflow_data:\n",
    "        raise ValueError(\"Cash flow statement not found in filing.\")\n",
    "\n",
    "    # Step 6: Parse income data from XML\n",
    "    income_data = parse_income_from_10k(xbrl_url)\n",
    "    if not income_data:\n",
    "        raise ValueError(\"income statement not found in filing.\")\n",
    "\n",
    "    # Step 7: Return results\n",
    "    return {\n",
    "        \"ticker\": ticker,\n",
    "        \"cik\": cik,\n",
    "        \"source\": xbrl_url,\n",
    "        \"cashflow\": cashflow_data,\n",
    "        \"income_statement\": income_data,\n",
    "        \"filing_date\": filing_date,\n",
    "        \"report_date\": report_date,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098a3fa6-50f5-4b0a-98e9-b94200faac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import firestore\n",
    "import json\n",
    "\n",
    "# Initialize Firestore client (ensure your JSON path is correct)\n",
    "db = firestore.Client.from_service_account_json(\"funwai-resume-firebase-adminsdk-fbsvc-a956eb6362.json\")\n",
    "\n",
    "def store_financials_to_firestore(financials_dict, collection_name=\"company_financials\"):\n",
    "    \"\"\"\n",
    "    Stores cashflow data from get_cashflow_json() into Firestore.\n",
    "    \n",
    "    Args:\n",
    "        financials_dict (dict): The result from get_financials_json(ticker)\n",
    "        collection_name (str): Firestore collection name\n",
    "    \"\"\"\n",
    "    ticker = financials_dict.get(\"ticker\")\n",
    "    if not ticker:\n",
    "        raise ValueError(\"Ticker not found in dictionary\")\n",
    "\n",
    "    # Use ticker as document ID (so each company overwrites with latest data)\n",
    "    doc_ref = db.collection(collection_name).document(ticker)\n",
    "    \n",
    "    # Store the data\n",
    "    doc_ref.set(financials_dict)\n",
    "    print(f\"‚úÖ Stored financial data for {ticker} in Firestore collection '{collection_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c98a1c-c66e-44e1-8fef-86eac51170cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to loop through all SP500 tickers and store if they are not in the firestore database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f105e7ca-6c30-4148-a363-2eaa366f04e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/320187/000032018725000047/0000320187-25-000047-index.html (filed 2025-07-17, for 2025-05-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/320187/000032018725000047/nke-20250531_htm.xml\n",
      "‚úÖ Stored financial data for NKE in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "## test the above function (it works!)\n",
    "data = get_cashflow_json(\"NKE\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97c87a03-30e8-4d8d-851f-99bf32bf4aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1065280/000106528025000044/0001065280-25-000044-index.html (filed 2025-01-27, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1065280/000106528025000044/nflx-20241231_htm.xml\n",
      "‚úÖ Stored financial data for NFLX in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"NFLX\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "934fafdc-df53-484a-bb91-26f1e8661036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1652044/000165204425000014/0001652044-25-000014-index.html (filed 2025-02-05, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1652044/000165204425000014/goog-20241231_htm.xml\n",
      "‚úÖ Stored financial data for GOOG in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"GOOG\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4498fbb-116c-4324-8e82-ccfed3d2fce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/789019/000095017025100235/0000950170-25-100235-index.html (filed 2025-07-30, for 2025-06-30)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/789019/000095017025100235/msft-20250630_htm.xml\n",
      "‚úÖ Stored financial data for MSFT in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"MSFT\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc2b72d3-8c20-4aa5-92c7-04f2aa959505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1326801/000132680125000017/0001326801-25-000017-index.html (filed 2025-01-30, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1326801/000132680125000017/meta-20241231_htm.xml\n",
      "‚úÖ Stored financial data for META in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1559720/000155972025000010/0001559720-25-000010-index.html (filed 2025-02-13, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1559720/000155972025000010/abnb-20241231_htm.xml\n",
      "‚úÖ Stored financial data for ABNB in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"META\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"ABNB\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2cde1aa7-53cb-4cef-bfd5-9340780542da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1679788/000167978825000022/0001679788-25-000022-index.html (filed 2025-02-13, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1679788/000167978825000022/coin-20241231_htm.xml\n",
      "‚úÖ Stored financial data for COIN in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"COIN\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2a1011ba-198c-4470-b035-509a9f81bfda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/2488/000000248825000012/0000002488-25-000012-index.html (filed 2025-02-05, for 2024-12-28)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/2488/000000248825000012/amd-20241228_htm.xml\n",
      "‚úÖ Stored financial data for AMD in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1713445/000171344525000018/0001713445-25-000018-index.html (filed 2025-02-13, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1713445/000171344525000018/rddt-20241231_htm.xml\n",
      "‚úÖ Stored financial data for RDDT in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/0001045810-25-000023-index.html (filed 2025-02-26, for 2025-01-26)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1045810/000104581025000023/nvda-20250126_htm.xml\n",
      "‚úÖ Stored financial data for NVDA in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/72971/000007297125000066/0000072971-25-000066-index.html (filed 2025-02-25, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/72971/000007297125000066/wfc-20241231_htm.xml\n",
      "‚úÖ Stored financial data for WFC in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/0000019617-25-000270-index.html (filed 2025-02-14, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/19617/000001961725000270/jpm-20241231_htm.xml\n",
      "‚úÖ Stored financial data for JPM in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/858877/000085887725000111/0000858877-25-000111-index.html (filed 2025-09-03, for 2025-07-26)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/858877/000085887725000111/csco-20250726_htm.xml\n",
      "‚úÖ Stored financial data for CSCO in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1166691/000116669125000011/0001166691-25-000011-index.html (filed 2025-01-31, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1166691/000116669125000011/cmcsa-20241231_htm.xml\n",
      "‚úÖ Stored financial data for CMCSA in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1403161/000140316124000058/0001403161-24-000058-index.html (filed 2024-11-13, for 2024-09-30)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1403161/000140316124000058/v-20240930_htm.xml\n",
      "‚úÖ Stored financial data for V in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1141391/000114139125000011/0001141391-25-000011-index.html (filed 2025-02-12, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1141391/000114139125000011/ma-20241231_htm.xml\n",
      "‚úÖ Stored financial data for MA in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/0000200406-25-000038-index.html (filed 2025-02-13, for 2024-12-29)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/200406/000020040625000038/jnj-20241229_htm.xml\n",
      "‚úÖ Stored financial data for JNJ in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/354950/000035495025000085/0000354950-25-000085-index.html (filed 2025-03-21, for 2025-02-02)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/354950/000035495025000085/hd-20250202_htm.xml\n",
      "‚úÖ Stored financial data for HD in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1543151/000154315125000008/0001543151-25-000008-index.html (filed 2025-02-14, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1543151/000154315125000008/uber-20241231_htm.xml\n",
      "‚úÖ Stored financial data for UBER in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/78003/000007800325000054/0000078003-25-000054-index.html (filed 2025-02-27, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/78003/000007800325000054/pfe-20241231_htm.xml\n",
      "‚úÖ Stored financial data for PFE in Firestore collection 'company_financials'.\n",
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/927628/000092762825000092/0000927628-25-000092-index.html (filed 2025-02-20, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/927628/000092762825000092/cof-20241231_htm.xml\n",
      "‚úÖ Stored financial data for COF in Firestore collection 'company_financials'.\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"AMD\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"RDDT\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"NVDA\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"WFC\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"JPM\")\n",
    "store_financials_to_firestore(data)\n",
    "                              \n",
    "data = get_cashflow_json(\"CSCO\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"CMCSA\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"V\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"MA\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"JNJ\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"HD\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"UBER\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"PFE\")\n",
    "store_financials_to_firestore(data)\n",
    "\n",
    "data = get_cashflow_json(\"COF\")\n",
    "store_financials_to_firestore(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2ed99-9f28-456e-b67a-5f846470aea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### New section ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1242162d-db1b-4531-8ef0-acbbc847e5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10-K filing: https://www.sec.gov/Archives/edgar/data/1679788/000167978825000022/0001679788-25-000022-index.html (filed 2025-02-13, for 2024-12-31)\n",
      "Using XBRL XML file: https://www.sec.gov/Archives/edgar/data/1679788/000167978825000022/coin-20241231_htm.xml\n",
      "{'ticker': 'COIN', 'cik': '0001679788', 'source': 'https://www.sec.gov/Archives/edgar/data/1679788/000167978825000022/coin-20241231_htm.xml', 'cashflow': {'Net profit (or loss if negative)': 2579066000.0, 'Depreciation (wear & tear on assets)': 127518000.0, 'stock_comp': 912838000.0, 'change_ar': None, 'change_inventory': None, 'change_ap': None, 'Cash from day-to-day business (Operating Cashflow)': 2556844000.0, 'Buying equipment/buildings (Capital Expenditure)': None, 'acquisitions': 0.0, 'asset_sales': None, 'investments_purchase': None, 'investments_maturity': None, 'Cash from investments (Buying/Selling assets)': -282385000.0, 'Money raised from issuing new shares': None, 'Money spent buying back shares of company': None, 'Borrowed money (New loans or bonds)': None, 'Loan repayments': 0.0, 'Dividends paid to shareholders': None, 'Cash from investors and loans (Financing activities)': 2828921000.0, 'Change in cash during the period': None, 'Cash at the beginning of the period': None, 'Cash remaining at the end of the period': 8543903000.0}, 'income_statement': {'revenue': {'Total Revenue': 6293246000.0, 'Advertising Revenue': None, 'Interest Income': None, 'Other Income': 29074000.0}, 'expenses': {'Cost of Revenue': None, 'Research & Development': 1468252000.0, 'Sales & Marketing': 654444000.0, 'General & Administrative': 1300257000.0, 'Operating Expenses (Total)': 4256868000.0, 'Interest Expense': None, 'Income Tax Expense': 363578000.0}, 'profit': {'Gross Profit': None, 'Operating Income': 2307160000.0, 'Income Before Tax': 2942644000.0, 'Net Income': 2579066000.0}, 'shares': {'Earnings per Share (Basic)': 10.42, 'Earnings per Share (Diluted)': 9.48, 'Weighted Average Shares Outstanding (Basic)': 247374000.0, 'Weighted Average Shares Outstanding (Diluted)': 273377000.0}}, 'filing_date': '2025-02-13', 'report_date': '2024-12-31'}\n"
     ]
    }
   ],
   "source": [
    "data = get_cashflow_json(\"COIN\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "382a8389-0732-4822-aa66-23b8babf2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# 1Ô∏è‚É£ Get the latest 10-K index page for a given company CIK\n",
    "def get_latest_10k_url(cik):\n",
    "    \"\"\"Retrieve latest 10-K filing document URL and dates for a given CIK\"\"\"\n",
    "    url = f\"{BASE}/submissions/CIK{cik}.json\"\n",
    "    res = requests.get(url, headers=HEADERS)\n",
    "    data = res.json()\n",
    "\n",
    "    for form, acc, filing_date, report_date in zip(\n",
    "        data[\"filings\"][\"recent\"][\"form\"],\n",
    "        data[\"filings\"][\"recent\"][\"accessionNumber\"],\n",
    "        data[\"filings\"][\"recent\"][\"filingDate\"],\n",
    "        data[\"filings\"][\"recent\"][\"reportDate\"]\n",
    "    ):\n",
    "        if form == \"10-K\":\n",
    "            acc_num = acc.replace(\"-\", \"\")\n",
    "            filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_num}/{acc}-index.html\"\n",
    "            return filing_url, filing_date, report_date\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "# 2Ô∏è‚É£ Use your HTML finder (your existing function)\n",
    "def get_primary_html_url(index_url):\n",
    "    \"\"\"\n",
    "    Find the main 10-K HTML document inside the filing's index page.\n",
    "    Returns the actual HTML file URL (not the inline XBRL viewer).\n",
    "    \"\"\"\n",
    "    res = requests.get(index_url, headers=HEADERS)\n",
    "    if res.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch index page: {res.status_code}\")\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    candidates = []\n",
    "\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        href = link[\"href\"].strip()\n",
    "        text = link.get_text(strip=True).lower()\n",
    "        href_lower = href.lower()\n",
    "\n",
    "        # Skip XMLs and exhibits\n",
    "        if any(x in href_lower for x in [\"_cal.xml\", \"_lab.xml\", \"_pre.xml\", \"_def.xml\", \".xml\"]):\n",
    "            continue\n",
    "        if \"exhibit\" in href_lower or \"ex\" in text:\n",
    "            continue\n",
    "\n",
    "        # Only consider HTML or Inline XBRL\n",
    "        if href_lower.endswith((\".htm\", \".html\")) or \"ix?doc=\" in href_lower:\n",
    "            score = 0\n",
    "            if \"10-k\" in href_lower or \"10k\" in href_lower:\n",
    "                score += 10\n",
    "            if \"ix?doc=\" in href_lower:\n",
    "                score += 20  # inline XBRL links usually indicate the primary document\n",
    "            if \"form\" in text or \"10-k\" in text:\n",
    "                score += 5\n",
    "            candidates.append((score, href))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Pick the top-scoring link\n",
    "    best_href = sorted(candidates, key=lambda x: x[0], reverse=True)[0][1]\n",
    "\n",
    "    # Normalize the URL ‚Äî convert inline XBRL to raw HTML\n",
    "    if \"ix?doc=\" in best_href:\n",
    "        best_href = best_href.split(\"ix?doc=\")[-1]\n",
    "        if not best_href.startswith(\"https://\"):\n",
    "            best_href = \"https://www.sec.gov\" + best_href\n",
    "    elif not best_href.startswith(\"http\"):\n",
    "        if best_href.startswith(\"/\"):\n",
    "            best_href = \"https://www.sec.gov\" + best_href\n",
    "        else:\n",
    "            best_href = \"https://www.sec.gov/\" + best_href\n",
    "\n",
    "    return best_href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d929e84-3640-49dd-b46b-dcad56f548f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert HTML content to markdown text (also used in notebook: RAG-api)\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import re\n",
    "\n",
    "def clean_10k_html(html_content):\n",
    "    \"\"\"Convert messy 10-K HTML into clean text.\"\"\"\n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    # Remove hidden and metadata elements\n",
    "    for tag in soup([\"script\", \"style\", \"ix:header\", \"ix:hidden\", \"link\", \"meta\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Keep only the visible content\n",
    "    visible_html = str(soup)\n",
    "\n",
    "    # Convert to readable Markdown-style text\n",
    "    text_maker = html2text.HTML2Text()\n",
    "    text_maker.ignore_links = True\n",
    "    text_maker.ignore_images = True\n",
    "    text_maker.body_width = 0  # No line wrapping\n",
    "    clean_text = text_maker.handle(visible_html)\n",
    "\n",
    "    # Normalize spaces\n",
    "    clean_text = re.sub(r\"\\n\\s*\\n\", \"\\n\\n\", clean_text)\n",
    "    return clean_text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8376a9a3-58bc-4b91-8880-f5a3ef1d5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "import requests\n",
    "import os\n",
    "# Make sure you actually load the .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "def store_10K_text_from_url(ticker, html_url, year):\n",
    "    \"\"\"\n",
    "    Fetch a 10-K HTML from a URL, convert it to cleaned text, and upload\n",
    "    the text to Firebase Storage under:\n",
    "    company_details/EDGAR (US)/filings/{ticker}_10K.txt\n",
    "    \"\"\"\n",
    "\n",
    "    # 1Ô∏è‚É£ Fetch the HTML content\n",
    "    headers = {\"User-Agent\": \"kurio-agent/1.0 (potatojacket9@gmail.com)\"}\n",
    "    res = requests.get(html_url, headers=headers)\n",
    "    if res.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch HTML from SEC: {res.status_code}\")\n",
    "    html_content = res.text\n",
    "\n",
    "    # 2Ô∏è‚É£ Convert HTML to cleaned plain text\n",
    "    try:\n",
    "        text_data = clean_10k_html(html_content)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to clean HTML for {ticker}: {e}\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Initialize Firebase Storage client\n",
    "    service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Service account file not found at {service_account_path}\"\n",
    "        )\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    # ‚úÖ Bucket name (must match your Firebase project)\n",
    "    bucket_name = \"funwai-resume.firebasestorage.app\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # 4Ô∏è‚É£ Path inside the bucket (TXT version)\n",
    "    blob_path = f\"company_details/EDGAR (US)/filings/{ticker}_{year}_10K.txt\"\n",
    "    blob = bucket.blob(blob_path)\n",
    "\n",
    "    # 5Ô∏è‚É£ Upload the cleaned text\n",
    "    blob.upload_from_string(text_data, content_type=\"text/plain\")\n",
    "\n",
    "    print(f\"‚úÖ Uploaded cleaned 10-K text for {ticker} to {blob_path}\")\n",
    "\n",
    "    # Optionally return a public URL (if your bucket allows)\n",
    "    return blob.public_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7321fbd-51c5-4252-b5ad-b6f67a0bf9bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_cik' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Use functions to store 10K as markdown text into firestore\u001b[39;00m\n\u001b[0;32m      2\u001b[0m ticker \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXOM\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m----> 3\u001b[0m cik \u001b[38;5;241m=\u001b[39m get_cik(ticker)\n\u001b[0;32m      4\u001b[0m filing_url, filing_date, report_date \u001b[38;5;241m=\u001b[39m get_latest_10k_url(cik)\n\u001b[0;32m      5\u001b[0m html_url \u001b[38;5;241m=\u001b[39m get_primary_html_url(filing_url)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_cik' is not defined"
     ]
    }
   ],
   "source": [
    "# Use functions to store 10K as markdown text into firestore\n",
    "ticker = \"XOM\" \n",
    "cik = get_cik(ticker)\n",
    "filing_url, filing_date, report_date = get_latest_10k_url(cik)\n",
    "html_url = get_primary_html_url(filing_url)\n",
    "print(html_url)\n",
    "data = store_10K_text_from_url(ticker, html_url, report_date[:4])\n",
    "print(data)\n",
    "print(\"HTML->text \" + \"for \" + ticker + \" stored at:\", html_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82beb4a-2d90-4d38-92c0-94e89d29fc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "print(service_account_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "163c98ab-a842-4a78-a085-7fd0852d7310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-31\n"
     ]
    }
   ],
   "source": [
    "print(filing_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65726e99-b856-44f9-af02-60d450c60441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001067983\n"
     ]
    }
   ],
   "source": [
    "cik = get_cik(\"BRK-B\")\n",
    "print(cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d4049d0a-99f0-4546-b908-b3bcebcfa6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking for .txt files in bucket...\n",
      "‚¨áÔ∏è Downloading AAPL_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ABBV_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ABNB_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ABT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ACN_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ADBE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ADI_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AEP_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AJG_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AMAT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AMCR_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AMD_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AME_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AMZN_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading AON_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading A_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BAC_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BALL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BBY_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BKNG_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BKR_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BMY_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BRK-B_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading BR_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CAH_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CAT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CBOE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CCL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CHD_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CI_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CLX_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CMCSA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CME_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CNP_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading COIN_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading COP_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading COST_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CPAY_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CRL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CRM_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CRWD_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CSCO_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CTRA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CTSH_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CVS_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading CVX_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading C_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DAL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DASH_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DDOG_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DD_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DELL_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DIS_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading DLTR_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading D_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EA_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EBAY_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EFX_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EL_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EOG_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ETN_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading EXPE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading FDS_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading FDX_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading FE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading FICO_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading F_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GDDY_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GIS_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GOOGL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GOOG_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading GS_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HAL_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HAS_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HCA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HD_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HLT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HON_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HPE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HSIC_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading HUM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading IBM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading ICE_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading INTC_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading INTU_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading IT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading JNJ_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading JPM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading J_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KDP_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KHC_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KIM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KKR_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KMB_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KMX_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KO_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading KR_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading K_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LH_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LIN_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LLY_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LMT_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LOW_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading LULU_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading L_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MAR_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MCD_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MCO_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MDLZ_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading META_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MET_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MMM_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MNST_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MOS_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MO_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MPWR_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MRNA_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MSCI_2024_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MSFT_2025_10K.txt ...\n",
      "‚¨áÔ∏è Downloading MS_2024_10K.txt ...\n",
      "‚úÖ Download complete. 125 files saved to ./clean_10k_texts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def download_all_10k_texts(local_output_folder, service_account_path=None):\n",
    "    \"\"\"\n",
    "    Downloads all .txt 10-K files stored in:\n",
    "    company_details/EDGAR (US)/filings/\n",
    "    and saves them into a local folder.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load service account path from .env if not provided\n",
    "    if service_account_path is None:\n",
    "        service_account_path = os.getenv(\"FIREBASE_SERVICE_ACCOUNT_JSON\")\n",
    "\n",
    "    if not service_account_path or not os.path.exists(service_account_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"‚ùå Service account file not found at: {service_account_path}\"\n",
    "        )\n",
    "\n",
    "    # Create Firebase Storage client\n",
    "    credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "    client = storage.Client(credentials=credentials, project=credentials.project_id)\n",
    "\n",
    "    bucket_name = \"funwai-resume.firebasestorage.app\"\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    # Folder where your cleaned .txt files live inside Firebase Storage\n",
    "    prefix = \"company_details/EDGAR (US)/filings/\"\n",
    "\n",
    "    # Ensure local output folder exists\n",
    "    os.makedirs(local_output_folder, exist_ok=True)\n",
    "\n",
    "    print(\"üîç Checking for .txt files in bucket...\")\n",
    "\n",
    "    # List all files under prefix\n",
    "    blobs = bucket.list_blobs(prefix=prefix)\n",
    "\n",
    "    download_count = 0\n",
    "\n",
    "    for blob in blobs:\n",
    "        if blob.name.endswith(\".txt\"):  # Only download .txt files\n",
    "            filename = blob.name.split(\"/\")[-1]\n",
    "            local_path = os.path.join(local_output_folder, filename)\n",
    "\n",
    "            print(f\"‚¨áÔ∏è Downloading {filename} ...\")\n",
    "\n",
    "            blob.download_to_filename(local_path)\n",
    "            download_count += 1\n",
    "\n",
    "    if download_count == 0:\n",
    "        print(\"‚ö†Ô∏è No .txt files found in the specified folder.\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Download complete. {download_count} files saved to {local_output_folder}\")\n",
    "\n",
    "\n",
    "# ---- Run the download ----\n",
    "\n",
    "download_all_10k_texts(\n",
    "    local_output_folder=\"./clean_10k_texts\"   # choose your folder\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b737657-41be-433d-b196-b3858cc85d29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
